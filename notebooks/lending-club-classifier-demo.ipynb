{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lending Club Classifier and .deploy() Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set-up running Spark 2.0 (required for this demo) from a Jupyter notebook, follow these [instructions](https://github.com/combust-ml/mleap/wiki/Setting-up-a-Spark-2.0-notebook-with-MLeap-and-Toree).\n",
    "\n",
    "This demo will show you how to:\n",
    "1. Load the research dataset from s3\n",
    "2. Construct a feature transformer pipeline using commonly available transformers in Spark\n",
    "3. Train and deploy our classifiers to a public model server hosted on the combust.ml cloud using .deploy()\n",
    "\n",
    "NOTE: To run the actual deploy step you have to either:\n",
    "1. Get a key from combust.ml - it's easy, just email us!\n",
    "2. Fire up the combust cloud server on your local machine - also easy, send us an email and we'll send you a docker image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for the demo was pulled together from the publicly available [Lending Club Statistics datasets](https://www.lendingclub.com/info/download-data.action). The original data provided by Lending Club (Issued and Rejected loans) is not standardized, so for this demo we've gone ahead and pulled together only the common set of fields for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load libraries and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we've made it so that you have to download the [data](https://s3-us-west-2.amazonaws.com/mleap-demo/datasources/lending_club.avro) from s3. We suggest that you place it in your /tmp directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// %AddDeps ml.combust.mleap mleap-spark_2.11 0.3.0-SNAPSHOT --transitive --repository file:///Users/mikhail/.m2/repository\n",
    "// %AddDeps com.databricks spark-avro_2.11 3.0.1\n",
    "\n",
    "// Spark Training Pipeline Libraries\n",
    "import org.apache.spark.ml.mleap.feature.OneHotEncoder\n",
    "import org.apache.spark.ml.feature.{StandardScaler, StringIndexer, VectorAssembler, PolynomialExpansion}\n",
    "import org.apache.spark.ml.classification.{RandomForestClassifier, LogisticRegression}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineStage}\n",
    "\n",
    "// MLeap/Bundle.ML Serialization Libraries\n",
    "import ml.combust.mleap.spark.SparkSupport._\n",
    "import resource._\n",
    "import ml.combust.bundle.BundleFile\n",
    "import org.apache.spark.ml.bundle.SparkBundleContext\n",
    "\n",
    "// Combust libraries to deploy model to combust cloud (optional)\n",
    "import ml.combust.model.client.spark.SparkSupport._\n",
    "import akka.actor.ActorSystem\n",
    "import akka.stream.ActorMaterializer\n",
    "import scala.concurrent.duration._\n",
    "import scala.concurrent.Await"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT!!! You may have to run this next block of code a few times to get it to work - this is due to another bug in Toree. For me, running it twice works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755473\n"
     ]
    }
   ],
   "source": [
    "// Step 1. Load our Lending Club dataset\n",
    "\n",
    "val inputFile = \"file:////tmp/lending_club.avro\"\n",
    "\n",
    "var dataset = spark.sqlContext.read.format(\"com.databricks.spark.avro\").\n",
    "  load(inputFile)\n",
    "\n",
    "println(dataset.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------+----------+-----+--------+------------------+\n",
      "|loan_amount|fico_score_group_fnl|   dti|emp_length|state|approved|        loan_title|\n",
      "+-----------+--------------------+------+----------+-----+--------+------------------+\n",
      "|     1000.0|           650 - 700|   0.1|   4 years|   NM|     0.0|Wedding/Engagement|\n",
      "|     1000.0|           700 - 800|   0.1|  < 1 year|   MA|     0.0|Debt Consolidation|\n",
      "|    11000.0|           700 - 800|   0.1|    1 year|   MD|     0.0|Debt Consolidation|\n",
      "|     6000.0|           650 - 700|0.3864|  < 1 year|   MA|     0.0|             Other|\n",
      "|     1500.0|           500 - 550|0.0943|  < 1 year|   MD|     0.0|             Other|\n",
      "+-----------+--------------------+------+----------+-----+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.select(\"loan_amount\", \"fico_score_group_fnl\", \"dti\", \"emp_length\", \"state\", \"approved\", \"loan_title\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cap DTI and Keep Select Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be available as a custom transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755473\n",
      "754604\n"
     ]
    }
   ],
   "source": [
    "dataset.registerTempTable(\"df\")\n",
    "println(dataset.count())\n",
    "\n",
    "val datasetFnl = spark.sqlContext.sql(f\"\"\"\n",
    "    select\n",
    "        loan_amount,\n",
    "        fico_score_group_fnl,\n",
    "        case when dti >= 10.0\n",
    "            then 10.0\n",
    "            else dti\n",
    "        end as dti,\n",
    "        emp_length,\n",
    "        case when state in ('CA', 'NY', 'MN', 'IL', 'FL', 'WA', 'MA', 'TX', 'GA', 'OH', 'NJ', 'VA', 'MI')\n",
    "            then state\n",
    "            else 'Other'\n",
    "        end as state,\n",
    "        loan_title,\n",
    "        approved\n",
    "    from df\n",
    "    where loan_title in('Debt Consolidation', 'Other', 'Home/Home Improvement', 'Payoff Credit Card', 'Car Payment/Loan',\n",
    "    'Business Loan', 'Health/Medical', 'Moving', 'Wedding/Engagement', 'Vacation', 'College', 'Renewable Energy', 'Payoff Bills',\n",
    "    'Personal Loan', 'Motorcycle')\n",
    "\"\"\")\n",
    "\n",
    "println(datasetFnl.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at some summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+-----+--------+\n",
      "|state|    n|loan_amount|  dti|approved|\n",
      "+-----+-----+-----------+-----+--------+\n",
      "|   CA|99793|   13687.90|14.38|    0.00|\n",
      "|   TX|62049|   13165.46| 8.94|    0.00|\n",
      "|   NY|60715|   13244.03|10.81|    0.00|\n",
      "|   FL|60051|   12488.73| 8.85|    0.00|\n",
      "|   PA|33167|   12776.74| 7.87|    0.00|\n",
      "|   IL|31487|   13224.26| 8.45|    0.00|\n",
      "|   GA|29000|   12362.53|11.25|    0.00|\n",
      "|   OH|28511|   12159.61| 7.90|    0.00|\n",
      "|   NJ|27665|   13935.15|10.38|    0.00|\n",
      "|   VA|23556|   12950.66|12.43|    0.00|\n",
      "|   MI|20696|   12641.24| 8.77|    0.00|\n",
      "|   NC|20389|   12588.16| 4.59|    0.00|\n",
      "|   MA|18808|   12456.19| 9.57|    0.00|\n",
      "|   MD|17859|   12100.79| 7.52|    0.00|\n",
      "|   AZ|16281|   12820.07| 9.81|    0.00|\n",
      "+-----+-----+-----------+-----+--------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Most popular cities (original dataset)\n",
    "\n",
    "spark.sqlContext.sql(f\"\"\"\n",
    "    select \n",
    "        state,\n",
    "        count(*) as n,\n",
    "        cast(avg(loan_amount) as decimal(12,2)) as loan_amount,\n",
    "        cast(avg(dti) as decimal(12,2)) as dti,\n",
    "        cast(avg(approved) as decimal(12,2)) as approved\n",
    "    from df\n",
    "    group by state\n",
    "    order by count(*) desc\n",
    "\"\"\").show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----------+-----+--------+\n",
      "|          loan_title|     n|loan_amount|  dti|approved|\n",
      "+--------------------+------+-----------+-----+--------+\n",
      "|  Debt Consolidation|293152|   14973.08| 2.99|    0.00|\n",
      "|               Other|179838|    9404.06|23.50|    0.00|\n",
      "|Home/Home Improve...| 59073|   15292.14| 2.92|    0.00|\n",
      "|  Payoff Credit Card| 57827|   16015.58| 4.29|    0.00|\n",
      "|    Car Payment/Loan| 46369|   10368.69| 3.49|    0.00|\n",
      "|       Business Loan| 41286|   18215.65|13.60|    0.00|\n",
      "|      Health/Medical| 20129|    7452.36| 4.80|    0.00|\n",
      "|              Moving| 18460|    6638.31|14.80|    0.00|\n",
      "|  Wedding/Engagement| 12867|   10197.83| 4.57|    0.00|\n",
      "|            Vacation|  9729|    5627.50| 6.62|    0.00|\n",
      "|             College|  7631|    7974.38|43.78|    0.00|\n",
      "|    Renewable Energy|  3165|    9794.15|11.15|    0.00|\n",
      "|        Payoff Bills|  2302|   10826.49| 1.56|    0.00|\n",
      "|       Personal Loan|  2256|    9496.62| 0.43|    0.00|\n",
      "|          Motorcycle|   520|    7651.01| 0.20|    0.00|\n",
      "+--------------------+------+-----------+-----+--------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Most popular cities (original dataset)\n",
    "\n",
    "spark.sqlContext.sql(f\"\"\"\n",
    "    select \n",
    "        loan_title,\n",
    "        count(*) as n,\n",
    "        cast(avg(loan_amount) as decimal(12,2)) as loan_amount,\n",
    "        cast(avg(dti) as decimal(12,2)) as dti,\n",
    "        cast(avg(approved) as decimal(12,2)) as approved\n",
    "    from df\n",
    "    group by loan_title\n",
    "    order by count(*) desc\n",
    "\"\"\").show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define continous and categorical features and filter nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Step 2. Create our feature pipeline and train it on the entire dataset\n",
    "val continuousFeatures = Array(\"loan_amount\",\n",
    "  \"dti\")\n",
    "\n",
    "val categoricalFeatures = Array(\"loan_title\",\n",
    "  \"emp_length\",\n",
    "  \"state\",\n",
    "  \"fico_score_group_fnl\")\n",
    "\n",
    "val allFeatures = continuousFeatures.union(categoricalFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "754604\n"
     ]
    }
   ],
   "source": [
    "// Filter all null values\n",
    "val allCols = allFeatures.union(Seq(\"approved\")).map(datasetFnl.col)\n",
    "val nullFilter = allCols.map(_.isNotNull).reduce(_ && _)\n",
    "val datasetImputedFiltered = datasetFnl.select(allCols: _*).filter(nullFilter).persist()\n",
    "\n",
    "println(datasetImputedFiltered.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Split data into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val Array(trainingDataset, validationDataset) = datasetImputedFiltered.randomSplit(Array(0.7, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Continous Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val continuousFeatureAssembler = new VectorAssembler(uid = \"continuous_feature_assembler\").\n",
    "    setInputCols(continuousFeatures).\n",
    "    setOutputCol(\"unscaled_continuous_features\")\n",
    "\n",
    "val continuousFeatureScaler = new StandardScaler(uid = \"continuous_feature_scaler\").\n",
    "    setInputCol(\"unscaled_continuous_features\").\n",
    "    setOutputCol(\"scaled_continuous_features\")\n",
    "\n",
    "val polyExpansionAssembler = new VectorAssembler(uid = \"poly_expansion_feature_assembler\").\n",
    "    setInputCols(Array(\"loan_amount\", \"dti\")).\n",
    "    setOutputCol(\"poly_expansions_features\")\n",
    "\n",
    "val continuousFeaturePolynomialExpansion = new PolynomialExpansion(uid = \"polynomial_expansion_loan_amount\").\n",
    "    setInputCol(\"poly_expansions_features\").\n",
    "    setOutputCol(\"loan_amount_polynomial_expansion_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Categorical Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val categoricalFeatureIndexers = categoricalFeatures.map {\n",
    "    feature => new StringIndexer(uid = s\"string_indexer_$feature\").\n",
    "      setInputCol(feature).\n",
    "      setOutputCol(s\"${feature}_index\")\n",
    "}\n",
    "\n",
    "val categoricalFeatureOneHotEncoders = categoricalFeatureIndexers.map {\n",
    "    indexer => new OneHotEncoder(uid = s\"oh_encoder_${indexer.getOutputCol}\").\n",
    "      setInputCol(indexer.getOutputCol).\n",
    "      setOutputCol(s\"${indexer.getOutputCol}_oh\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Assemble our features and feature pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished constructing the pipeline\n"
     ]
    }
   ],
   "source": [
    "val featureColsRf = categoricalFeatureIndexers.map(_.getOutputCol).union(Seq(\"scaled_continuous_features\", \"loan_amount_polynomial_expansion_features\"))\n",
    "val featureColsLr = categoricalFeatureOneHotEncoders.map(_.getOutputCol).union(Seq(\"scaled_continuous_features\"))\n",
    "\n",
    "// assemble all processes categorical and continuous features into a single feature vector\n",
    "val featureAssemblerLr = new VectorAssembler(uid = \"feature_assembler_lr\").\n",
    "    setInputCols(featureColsLr).\n",
    "    setOutputCol(\"features_lr\")\n",
    "    \n",
    "val featureAssemblerRf = new VectorAssembler(uid = \"feature_assembler_rf\").\n",
    "    setInputCols(featureColsRf).\n",
    "    setOutputCol(\"features_rf\")\n",
    "\n",
    "val estimators: Array[PipelineStage] = Array(continuousFeatureAssembler, continuousFeatureScaler, polyExpansionAssembler, continuousFeaturePolynomialExpansion).\n",
    "    union(categoricalFeatureIndexers).\n",
    "    union(categoricalFeatureOneHotEncoders).\n",
    "    union(Seq(featureAssemblerLr, featureAssemblerRf))\n",
    "\n",
    "val featurePipeline = new Pipeline(uid = \"feature_pipeline\").\n",
    "    setStages(estimators)\n",
    "val sparkFeaturePipelineModel = featurePipeline.fit(datasetImputedFiltered)\n",
    "\n",
    "println(\"Finished constructing the pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete: Training Random Forest\n"
     ]
    }
   ],
   "source": [
    "// Create our random forest model\n",
    "val randomForest = new RandomForestClassifier(uid = \"random_forest_classifier\").\n",
    "    setFeaturesCol(\"features_rf\").\n",
    "    setLabelCol(\"approved\").\n",
    "    setPredictionCol(\"approved_prediction\")\n",
    "\n",
    "val sparkPipelineEstimatorRf = new Pipeline().setStages(Array(sparkFeaturePipelineModel, randomForest))\n",
    "val sparkPipelineRf = sparkPipelineEstimatorRf.fit(datasetImputedFiltered)\n",
    "\n",
    "println(\"Complete: Training Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Train Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete: Training Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "val logisticRegression = new LogisticRegression(uid = \"logistic_regression\").\n",
    "    setFeaturesCol(\"features_lr\").\n",
    "    setLabelCol(\"approved\").\n",
    "    setPredictionCol(\"approved_prediction\")\n",
    "\n",
    "val sparkPipelineEstimatorLr = new Pipeline().setStages(Array(sparkFeaturePipelineModel, logisticRegression))\n",
    "val sparkPipelineLr = sparkPipelineEstimatorLr.fit(datasetImputedFiltered)\n",
    "\n",
    "println(\"Complete: Training Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Set up the ActorSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "implicit val system = ActorSystem(\"combust-client\")\n",
    "implicit val materializer = ActorMaterializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Deploy your LR and RF pipelines to the model servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadModelResponse(ResourceAlias(ResourceKey(my_username,lc_model_rf,models),Some(Resource(88d169f9-3971-44ae-8f11-f343becfedda,models))))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// {\n",
    "//     implicit val context = sbc\n",
    "//     Await.result(sparkPipelineLr.deploy(\"http://models.combust.ml:65327\", \"my_username\", \"lc_model_lr\", token), 10.seconds)\n",
    "// }\n",
    "// {\n",
    "//     implicit val context = sbcRf\n",
    "//     Await.result(sparkPipelineRf.deploy(\"http://models.combust.ml:65327\", \"my_username\", \"lc_model_rf\", token), 10.seconds)\n",
    "// }\n",
    "\n",
    "\n",
    "{\n",
    "    implicit val context = sbc\n",
    "    Await.result(sparkPipelineLr.deploy(\"http://localhost:65327\", \"my_username\", \"lc_model_lr\"), 10.seconds)\n",
    "}\n",
    "{\n",
    "    implicit val context = sbcRf\n",
    "    Await.result(sparkPipelineRf.deploy(\"http://localhost:65327\", \"my_username\", \"lc_model_rf\"), 10.seconds)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11 (Optional): Serialize your models to bundle.ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val sbc = SparkBundleContext().withDataset(sparkPipelineLr.transform(datasetImputedFiltered))\n",
    "for(bf <- managed(BundleFile(\"jar:file:/tmp/lc.model.lr.zip\"))) {\n",
    "        sparkPipelineLr.writeBundle.save(bf)(sbc).get\n",
    "      }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val sbcRf = SparkBundleContext().withDataset(sparkPipelineRf.transform(datasetImputedFiltered))\n",
    "for(bf <- managed(BundleFile(\"jar:file:/tmp/lc.model.rf.zip\"))) {\n",
    "        sparkPipelineRf.writeBundle.save(bf)(sbcRf).get\n",
    "      }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala2",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
