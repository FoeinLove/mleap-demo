{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lending Club Classifier and .deploy() Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set-up running Spark 2.0 (required for this demo) from a Jupyter notebook, follow these [instructions](https://github.com/combust-ml/mleap/wiki/Setting-up-a-Spark-2.0-notebook-with-MLeap-and-Toree).\n",
    "\n",
    "This demo will show you how to:\n",
    "1. Load the research dataset from s3\n",
    "2. Construct a feature transformer pipeline using commonly available transformers in Spark\n",
    "3. Train and deploy our classifiers to a public model server hosted on the combust.ml cloud using .deploy()\n",
    "\n",
    "NOTE: To run the actual deploy step you have to either:\n",
    "1. Get a key from combust.ml - it's easy, just email us!\n",
    "2. Fire up the combust cloud server on your local machine - also easy, send us an email and we'll send you a docker image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for the demo was pulled together from the publicly available [Lending Club Statistics datasets](https://www.lendingclub.com/info/download-data.action). The original data provided by Lending Club (Issued and Rejected loans) is not standardized, so for this demo we've gone ahead and pulled together only the common set of fields for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Load libraries and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we've made it so that you have to download the [data](https://s3-us-west-2.amazonaws.com/mleap-demo/datasources/airbnb.avro.zip) from s3. We suggest that you place it in your /tmp directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once [TOREE-345](https://issues.apache.org/jira/browse/TOREE-345) is fixed, we won't have to deal with complicated notebook setup and will just be able to include:\n",
    "\n",
    "```scala\n",
    "%AddDeps ml.combust.mleap mleap-spark_2.11 0.3.0 --transitive\n",
    "```\n",
    "\n",
    "For now, make sure to clone [mleap](https://github.com/combust-ml/mleap) and run:\n",
    "\n",
    "```bash\n",
    "sbt \"+ publishLocal\"\n",
    "sbt publishM2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking ml.combust.mleap:mleap-spark_2.11:0.3.0-SNAPSHOT for download\n",
      "Preparing to fetch from:\n",
      "-> file:/var/folders/b2/swftcs0s4bvfpqfvn3rm0pzw0000gn/T/toree_add_deps2959412719271534823/\n",
      "-> file:/Users/mikhail/.m2/repository\n",
      "-> https://repo1.maven.org/maven2\n",
      "-> New file at /var/folders/b2/swftcs0s4bvfpqfvn3rm0pzw0000gn/T/toree_add_deps2959412719271534823/https/repo1.maven.org/maven2/com/lihaoyi/sourcecode_2.11/0.1.1/sourcecode_2.11-0.1.1.jar\n",
      "-> New file at /var/folders/b2/swftcs0s4bvfpqfvn3rm0pzw0000gn/T/toree_add_deps2959412719271534823/https/repo1.maven.org/maven2/io/spray/spray-json_2.11/1.3.2/spray-json_2.11-1.3.2.jar\n",
      "-> New file at /var/folders/b2/swftcs0s4bvfpqfvn3rm0pzw0000gn/T/toree_add_deps2959412719271534823/https/repo1.maven.org/maven2/com/lihaoyi/fastparse-utils_2.11/0.3.7/fastparse-utils_2.11-0.3.7.jar\n",
      "-> New file at /Users/mikhail/.m2/repository/ml/combust/mleap/mleap-runtime_2.11/0.3.0-SNAPSHOT/mleap-runtime_2.11-0.3.0-SNAPSHOT.jar\n",
      "-> New file at /Users/mikhail/.m2/repository/ml/combust/mleap/mleap-spark_2.11/0.3.0-SNAPSHOT/mleap-spark_2.11-0.3.0-SNAPSHOT.jar\n",
      "-> New file at /var/folders/b2/swftcs0s4bvfpqfvn3rm0pzw0000gn/T/toree_add_deps2959412719271534823/https/repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.0.0/protobuf-java-3.0.0.jar\n",
      "-> New file at /var/folders/b2/swftcs0s4bvfpqfvn3rm0pzw0000gn/T/toree_add_deps2959412719271534823/https/repo1.maven.org/maven2/com/trueaccord/lenses/lenses_2.11/0.4.6/lenses_2.11-0.4.6.jar\n",
      "-> New file at /var/folders/b2/swftcs0s4bvfpqfvn3rm0pzw0000gn/T/toree_add_deps2959412719271534823/https/repo1.maven.org/maven2/com/lihaoyi/fastparse_2.11/0.3.7/fastparse_2.11-0.3.7.jar\n",
      "-> New file at /Users/mikhail/.m2/repository/ml/combust/mleap/mleap-core_2.11/0.3.0-SNAPSHOT/mleap-core_2.11-0.3.0-SNAPSHOT.jar\n",
      "-> New file at /var/folders/b2/swftcs0s4bvfpqfvn3rm0pzw0000gn/T/toree_add_deps2959412719271534823/https/repo1.maven.org/maven2/com/jsuereth/scala-arm_2.11/2.0-RC1/scala-arm_2.11-2.0-RC1.jar\n",
      "-> New file at /var/folders/b2/swftcs0s4bvfpqfvn3rm0pzw0000gn/T/toree_add_deps2959412719271534823/https/repo1.maven.org/maven2/com/typesafe/config/1.3.0/config-1.3.0.jar\n",
      "-> New file at /Users/mikhail/.m2/repository/ml/combust/bundle/bundle-ml_2.11/0.3.0-SNAPSHOT/bundle-ml_2.11-0.3.0-SNAPSHOT.jar\n",
      "-> New file at /var/folders/b2/swftcs0s4bvfpqfvn3rm0pzw0000gn/T/toree_add_deps2959412719271534823/https/repo1.maven.org/maven2/com/trueaccord/scalapb/scalapb-runtime_2.11/0.5.42/scalapb-runtime_2.11-0.5.42.jar\n",
      "Marking com.databricks:spark-avro_2.11:3.0.1 for download\n",
      "Preparing to fetch from:\n",
      "-> file:/var/folders/b2/swftcs0s4bvfpqfvn3rm0pzw0000gn/T/toree_add_deps2959412719271534823/\n",
      "-> https://repo1.maven.org/maven2\n",
      "-> New file at /var/folders/b2/swftcs0s4bvfpqfvn3rm0pzw0000gn/T/toree_add_deps2959412719271534823/https/repo1.maven.org/maven2/com/databricks/spark-avro_2.11/3.0.1/spark-avro_2.11-3.0.1.jar\n"
     ]
    }
   ],
   "source": [
    "%AddDeps ml.combust.mleap mleap-spark_2.11 0.3.0-SNAPSHOT --transitive --repository file:///Users/mikhail/.m2/repository\n",
    "%AddDeps com.databricks spark-avro_2.11 3.0.1\n",
    "\n",
    "import org.apache.spark.ml.mleap.feature.OneHotEncoder\n",
    "import org.apache.spark.ml.feature.{StandardScaler, StringIndexer, VectorAssembler}\n",
    "import org.apache.spark.ml.classification.{RandomForestClassifier, LogisticRegression}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineStage}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT!!! You may have to run this next block of code a few times to get it to work - this is due to another bug in Toree. For me, running it twice works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755473\n"
     ]
    }
   ],
   "source": [
    "// Step 1. Load our Lending Club dataset\n",
    "\n",
    "val inputFile = \"file:////tmp/lending_club.avro\"\n",
    "val outputFileRf = \"/tmp/transformer.rf.ml\"\n",
    "val outputFileLr = \"/tmp/transformer.lr.ml\"\n",
    "\n",
    "var dataset = spark.sqlContext.read.format(\"com.databricks.spark.avro\").\n",
    "  load(inputFile)\n",
    "\n",
    "println(dataset.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------+----------+-----+--------+------------------+\n",
      "|loan_amount|fico_score_group_fnl|   dti|emp_length|state|approved|        loan_title|\n",
      "+-----------+--------------------+------+----------+-----+--------+------------------+\n",
      "|     1000.0|           650 - 700|   0.1|   4 years|   NM|     0.0|Wedding/Engagement|\n",
      "|     1000.0|           700 - 800|   0.1|  < 1 year|   MA|     0.0|Debt Consolidation|\n",
      "|    11000.0|           700 - 800|   0.1|    1 year|   MD|     0.0|Debt Consolidation|\n",
      "|     6000.0|           650 - 700|0.3864|  < 1 year|   MA|     0.0|             Other|\n",
      "|     1500.0|           500 - 550|0.0943|  < 1 year|   MD|     0.0|             Other|\n",
      "+-----------+--------------------+------+----------+-----+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.select(\"loan_amount\", \"fico_score_group_fnl\", \"dti\", \"emp_length\", \"state\", \"approved\", \"loan_title\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cap DTI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be available as a custom transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.registerTempTable(\"df\")\n",
    "\n",
    "val datasetFnl = spark.sqlContext.sql(f\"\"\"\n",
    "    select\n",
    "        loan_amount,\n",
    "        fico_score_group_fnl,\n",
    "        case when dti >= 10.0\n",
    "            then 10.0\n",
    "            else dti\n",
    "        end as dti,\n",
    "        emp_length,\n",
    "        state,\n",
    "        loan_title,\n",
    "        approved\n",
    "    from df\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at some summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+-----+--------+\n",
      "|state|    n|loan_amount|  dti|approved|\n",
      "+-----+-----+-----------+-----+--------+\n",
      "|   CA|99793|   13687.90|14.38|    0.00|\n",
      "|   TX|62049|   13165.46| 8.94|    0.00|\n",
      "|   NY|60715|   13244.03|10.81|    0.00|\n",
      "|   FL|60051|   12488.73| 8.85|    0.00|\n",
      "|   PA|33167|   12776.74| 7.87|    0.00|\n",
      "|   IL|31487|   13224.26| 8.45|    0.00|\n",
      "|   GA|29000|   12362.53|11.25|    0.00|\n",
      "|   OH|28511|   12159.61| 7.90|    0.00|\n",
      "|   NJ|27665|   13935.15|10.38|    0.00|\n",
      "|   VA|23556|   12950.66|12.43|    0.00|\n",
      "|   MI|20696|   12641.24| 8.77|    0.00|\n",
      "|   NC|20389|   12588.16| 4.59|    0.00|\n",
      "|   MA|18808|   12456.19| 9.57|    0.00|\n",
      "|   MD|17859|   12100.79| 7.52|    0.00|\n",
      "|   AZ|16281|   12820.07| 9.81|    0.00|\n",
      "+-----+-----+-----------+-----+--------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Most popular cities (original dataset)\n",
    "\n",
    "spark.sqlContext.sql(f\"\"\"\n",
    "    select \n",
    "        state,\n",
    "        count(*) as n,\n",
    "        cast(avg(loan_amount) as decimal(12,2)) as loan_amount,\n",
    "        cast(avg(dti) as decimal(12,2)) as dti,\n",
    "        cast(avg(approved) as decimal(12,2)) as approved\n",
    "    from df\n",
    "    group by state\n",
    "    order by count(*) desc\n",
    "\"\"\").show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----------+-----+--------+\n",
      "|          loan_title|     n|loan_amount|  dti|approved|\n",
      "+--------------------+------+-----------+-----+--------+\n",
      "|  Debt Consolidation|293152|   14973.08| 2.99|    0.00|\n",
      "|               Other|179838|    9404.06|23.50|    0.00|\n",
      "|Home/Home Improve...| 59073|   15292.14| 2.92|    0.00|\n",
      "|  Payoff Credit Card| 57827|   16015.58| 4.29|    0.00|\n",
      "|    Car Payment/Loan| 46369|   10368.69| 3.49|    0.00|\n",
      "|       Business Loan| 41286|   18215.65|13.60|    0.00|\n",
      "|      Health/Medical| 20129|    7452.36| 4.80|    0.00|\n",
      "|              Moving| 18460|    6638.31|14.80|    0.00|\n",
      "|  Wedding/Engagement| 12867|   10197.83| 4.57|    0.00|\n",
      "|            Vacation|  9729|    5627.50| 6.62|    0.00|\n",
      "|             College|  7631|    7974.38|43.78|    0.00|\n",
      "|    Renewable Energy|  3165|    9794.15|11.15|    0.00|\n",
      "|        Payoff Bills|  2302|   10826.49| 1.56|    0.00|\n",
      "|       Personal Loan|  2256|    9496.62| 0.43|    0.00|\n",
      "|          Motorcycle|   520|    7651.01| 0.20|    0.00|\n",
      "+--------------------+------+-----------+-----+--------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Most popular cities (original dataset)\n",
    "\n",
    "spark.sqlContext.sql(f\"\"\"\n",
    "    select \n",
    "        loan_title,\n",
    "        count(*) as n,\n",
    "        cast(avg(loan_amount) as decimal(12,2)) as loan_amount,\n",
    "        cast(avg(dti) as decimal(12,2)) as dti,\n",
    "        cast(avg(approved) as decimal(12,2)) as approved\n",
    "    from df\n",
    "    group by loan_title\n",
    "    order by count(*) desc\n",
    "\"\"\").show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define continous and categorical features and filter nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Step 2. Create our feature pipeline and train it on the entire dataset\n",
    "val continuousFeatures = Array(\"loan_amount\",\n",
    "  \"dti\")\n",
    "\n",
    "val categoricalFeatures = Array(\"loan_title\",\n",
    "  \"emp_length\",\n",
    "  \"state\",\n",
    "  \"fico_score_group_fnl\")\n",
    "\n",
    "val allFeatures = continuousFeatures.union(categoricalFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Filter all null values\n",
    "val allCols = allFeatures.union(Seq(\"approved\")).map(datasetImputed.col)\n",
    "val nullFilter = allCols.map(_.isNotNull).reduce(_ && _)\n",
    "val datasetImputedFiltered = datasetImputed.select(allCols: _*).filter(nullFilter).persist()\n",
    "\n",
    "println(datasetImputedFiltered.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Split data into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val Array(trainingDataset, validationDataset) = datasetImputedFiltered.randomSplit(Array(0.7, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Continous Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val continuousFeatureAssembler = new VectorAssembler(uid = \"continuous_feature_assembler\").\n",
    "    setInputCols(continuousFeatures).\n",
    "    setOutputCol(\"unscaled_continuous_features\")\n",
    "\n",
    "val continuousFeatureScaler = new StandardScaler(uid = \"continuous_feature_scaler\").\n",
    "    setInputCol(\"unscaled_continuous_features\").\n",
    "    setOutputCol(\"scaled_continuous_features\")\n",
    "\n",
    "val ContinuousFeaturePolynomialExpansion = new PolynomialExpansion(uid = \"polynomial_expansion_loan_amount\").\n",
    "    setInputCols(\"loan_amount\").\n",
    "    setOutputCol(\"loan_amount_polynomial_expansion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Categorical Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val categoricalFeatureIndexers = categoricalFeatures.map {\n",
    "    feature => new StringIndexer(uid = s\"string_indexer_$feature\").\n",
    "      setInputCol(feature).\n",
    "      setOutputCol(s\"${feature}_index\")\n",
    "}\n",
    "\n",
    "val categoricalFeatureOneHotEncoders = categoricalFeatureIndexers.map {\n",
    "    indexer => new OneHotEncoder(uid = s\"oh_encoder_${indexer.getOutputCol}\").\n",
    "      setInputCol(indexer.getOutputCol).\n",
    "      setOutputCol(s\"${indexer.getOutputCol}_oh\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Assemble our features and feature pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val featureColsRf = categoricalFeatureIndexers.map(_.getOutputCol).union(Seq(\"scaled_continuous_features\", \"loan_amount_polynomial_expansion\"))\n",
    "val featureColsLr = categoricalFeatureOneHotEncoders.map(_.getOutputCol).union(Seq(\"scaled_continuous_features\"))\n",
    "\n",
    "// assemble all processes categorical and continuous features into a single feature vector\n",
    "val featureAssemblerLr = new VectorAssembler(uid = \"feature_assembler_lr\").\n",
    "    setInputCols(featureColsLr).\n",
    "    setOutputCol(\"features_lr\")\n",
    "    \n",
    "val featureAssemblerRf = new VectorAssembler(uid = \"feature_assembler_rf\").\n",
    "    setInputCols(featureColsRf).\n",
    "    setOutputCol(\"features_rf\")\n",
    "\n",
    "val estimators: Array[PipelineStage] = Array(continuousFeatureAssembler, continuousFeatureScaler).\n",
    "    union(categoricalFeatureIndexers).\n",
    "    union(categoricalFeatureOneHotEncoders).\n",
    "    union(Seq(featureAssemblerLr, featureAssemblerRf))\n",
    "\n",
    "val featurePipeline = new Pipeline(uid = \"feature_pipeline\").\n",
    "    setStages(estimators)\n",
    "val sparkFeaturePipelineModel = featurePipeline.fit(datasetImputedFiltered)\n",
    "\n",
    "println(\"Finished constructing the pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Step 3.1 Create our random forest model\n",
    "val randomForest = new RandomForestClassifier(uid = \"random_forest_classifier\").\n",
    "    setFeaturesCol(\"features_rf\").\n",
    "    setLabelCol(\"approved\").\n",
    "    setPredictionCol(\"approved_prediction\")\n",
    "\n",
    "val sparkPipelineEstimatorRf = new Pipeline().setStages(Array(sparkFeaturePipelineModel, randomForest))\n",
    "val sparkPipelineRf = sparkPipelineEstimatorRf.fit(datasetImputedFiltered)\n",
    "\n",
    "println(\"Complete: Training Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Train Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val logisticRegression = new LogisticRegression(uid = \"logistic_regression\").\n",
    "    setFeaturesCol(\"features_lr\").\n",
    "    setLabelCol(\"approved\").\n",
    "    setPredictionCol(\"approved_prediction\")\n",
    "\n",
    "val sparkPipelineEstimatorLr = new Pipeline().setStages(Array(sparkFeaturePipelineModel, logisticRegression))\n",
    "val sparkPipelineLr = sparkPipelineEstimatorLr.fit(datasetImputedFiltered)\n",
    "\n",
    "println(\"Complete: Training Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Load the libaries for .deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ml.combust.bundle.BundleRegistry\n",
    "import ml.combust.mleap.spark.SparkSupport._\n",
    "import ml.combust.client.spark.SparkSupport._\n",
    "import ml.combust.client.model.MultiModelClient\n",
    "import akka.actor.ActorSystem\n",
    "import akka.stream.ActorMaterializer\n",
    "import scala.concurrent.duration._\n",
    "import scala.concurrent.Await\n",
    "\n",
    "implicit val hr = BundleRegistry(\"spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Set up the ActorSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "implicit val system = ActorSystem(\"combust-client\")\n",
    "implicit val materializer = ActorMaterializer()\n",
    "implicit val ec = system.dispatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Define model servers to send your models to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val client = MultiModelClient.builder[String]().\n",
    "  withServer(\"model-server-1\", \"http://combust-model-server-4671c502.124d9c7a.svc.dockerapp.io:8889\").\n",
    "  withServer(\"model-server-2\", \"http://combust-model-server-9ac83b2d.801b9d51.svc.dockerapp.io:8890\").\n",
    "  build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Deploy your LR and RF pipelines to the model servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Await.result(client.deploy(sparkPipelineLr, \"airbnb_lr\").sequence, 10.seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Await.result(client.deploy(sparkPipelineRf, \"airbnb_rf\").sequence, 10.seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13 (Optional): Serialize your models to bundle.ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkPipelineLr.serializeToBundle(new java.io.File(\"/tmp/model.lr\"))\n",
    "sparkPipelineRf.serializeToBundle(new java.io.File(\"/tmp/model.rf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14 (Optional): Undeploy your models from the model server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Await.result(client.undeploy(\"airbnb_rf\").sequence, 10.seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Await.result(client.undeploy(\"airbnb_lr\").sequence, 10.seconds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
