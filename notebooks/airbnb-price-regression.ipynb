{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLeap.deploy() Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set-up running Spark 2.0 (required for this demo) from a Jupyter notebook, follow these [instructions](https://github.com/combust-ml/mleap/wiki/Setting-up-a-Spark-2.0-notebook-with-MLeap-and-Toree).\n",
    "\n",
    "This demo will show you how to:\n",
    "1. Load the research dataset from s3\n",
    "2. Construct a feature transformer pipeline using commonly available transformers in Spark\n",
    "3. Deploy your model to a public model server hosted on the combust.ml cloud using .deploy()\n",
    "\n",
    "NOTE: To run the actual deploy step you have to either:\n",
    "1. Get a key from combust.ml - it's easy, just email us!\n",
    "2. Fire up the combust cloud server on your local machine - also easy, send us an email and we'll send you a docker image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for the demo was pulled together from individual cities' data found [here](http://insideairbnb.com/get-the-data.html). We've also gone ahead and pulled the individual datasets and relevant features into this [research dataset](https://s3-us-west-2.amazonaws.com/mleap-demo/datasources/airbnb.avro.zip) stored as avro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Load libraries and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we've made it so that you have to download the [data]() from s3. We suggest that you unzip it in your /tmp directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// %AddDeps ml.combust.mleap mleap-spark_2.11 0.3.0 --transitive\n",
    "\n",
    "import org.apache.spark.ml.mleap.feature.OneHotEncoder\n",
    "import org.apache.spark.ml.feature.{StandardScaler, StringIndexer, VectorAssembler}\n",
    "import org.apache.spark.ml.regression.{RandomForestRegressor, LinearRegression}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineStage}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT!!! You may have to run this next block of code a few times to get it to work - this is due to another bug in Toree. For me, running it twice works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389255\n",
      "321588\n"
     ]
    }
   ],
   "source": [
    "// Step 1. Load our Airbnb dataset\n",
    "\n",
    "val inputFile = \"file:////tmp/airbnb.avro\"\n",
    "val outputFileRf = \"/tmp/transformer.rf.ml\"\n",
    "val outputFileLr = \"/tmp/transformer.lr.ml\"\n",
    "\n",
    "var dataset = spark.sqlContext.read.format(\"com.databricks.spark.avro\").\n",
    "  load(inputFile)\n",
    "\n",
    "var datasetFiltered = dataset.filter(\"price >= 50 AND price <= 750 and bathrooms > 0.0\")\n",
    "println(dataset.count())\n",
    "println(datasetFiltered.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Standardize the data for our demo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|       square_feet|             price|          bedrooms|         bathrooms|     cleaning_fee|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|            321588|            321588|            321588|            321588|           321588|\n",
      "|   mean| 546.7441757777032|131.54961006007687|1.3352426085550455| 1.199068373198005|37.64188340360959|\n",
      "| stddev|363.39839582374066| 90.10912788720098|0.8466586601060778|0.4830590051262673|42.64237791484579|\n",
      "|    min|             104.0|              50.0|               0.0|               0.5|              0.0|\n",
      "|    max|           32292.0|             750.0|              10.0|               8.0|            700.0|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasetFiltered.registerTempTable(\"df\")\n",
    "\n",
    "val datasetImputed = spark.sqlContext.sql(f\"\"\"\n",
    "    select\n",
    "        id,\n",
    "        city,\n",
    "        case when state in('NY', 'CA', 'London', 'Berlin', 'TX' ,'IL', 'OR', 'DC', 'WA')\n",
    "            then state\n",
    "            else 'Other'\n",
    "        end as state,\n",
    "        space,\n",
    "        price,\n",
    "        bathrooms,\n",
    "        bedrooms,\n",
    "        room_type,\n",
    "        host_is_superhost,\n",
    "        cancellation_policy,\n",
    "        case when security_deposit is null\n",
    "            then 0.0\n",
    "            else security_deposit\n",
    "        end as security_deposit,\n",
    "        price_per_bedroom,\n",
    "        case when number_of_reviews is null\n",
    "            then 0.0\n",
    "            else number_of_reviews\n",
    "        end as number_of_reviews,\n",
    "        case when extra_people is null\n",
    "            then 0.0\n",
    "            else extra_people\n",
    "        end as extra_people,\n",
    "        instant_bookable,\n",
    "        case when cleaning_fee is null\n",
    "            then 0.0\n",
    "            else cleaning_fee\n",
    "        end as cleaning_fee,\n",
    "        case when review_scores_rating is null\n",
    "            then 80.0\n",
    "            else review_scores_rating\n",
    "        end as review_scores_rating,\n",
    "        case when square_feet is not null and square_feet > 100\n",
    "            then square_feet\n",
    "            when (square_feet is null or square_feet <=100) and (bedrooms is null or bedrooms = 0)\n",
    "            then 350.0\n",
    "            else 380 * bedrooms\n",
    "        end as square_feet\n",
    "    from df\n",
    "    where bedrooms is not null\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "datasetImputed.select(\"square_feet\", \"price\", \"bedrooms\", \"bathrooms\", \"cleaning_fee\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Take a look at some summary statistics of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+---------+---------+\n",
      "|        state|    n|avg_price|max_price|\n",
      "+-------------+-----+---------+---------+\n",
      "|           NY|48362|   146.75|    750.0|\n",
      "|           CA|44716|   158.76|    750.0|\n",
      "|Île-de-France|40732|   107.74|    750.0|\n",
      "|       London|17532|   117.71|    750.0|\n",
      "|          NSW|14416|   167.96|    750.0|\n",
      "|       Berlin|13098|    81.01|    650.0|\n",
      "|Noord-Holland| 8890|   128.56|    750.0|\n",
      "|          VIC| 8636|   144.49|    750.0|\n",
      "|North Holland| 7636|   134.60|    700.0|\n",
      "|           IL| 7544|   141.85|    750.0|\n",
      "|           ON| 7186|   129.05|    750.0|\n",
      "|           TX| 6702|   196.59|    750.0|\n",
      "|           WA| 5858|   132.48|    750.0|\n",
      "|    Catalonia| 5748|   106.39|    720.0|\n",
      "|           BC| 5522|   133.14|    750.0|\n",
      "|           DC| 5476|   136.56|    720.0|\n",
      "|       Québec| 5116|   104.98|    700.0|\n",
      "|    Catalunya| 4570|    99.36|    675.0|\n",
      "|       Veneto| 4486|   131.71|    700.0|\n",
      "|           OR| 4330|   114.02|    700.0|\n",
      "+-------------+-----+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Most popular cities (original dataset)\n",
    "\n",
    "spark.sqlContext.sql(f\"\"\"\n",
    "    select \n",
    "        state,\n",
    "        count(*) as n,\n",
    "        cast(avg(price) as decimal(12,2)) as avg_price,\n",
    "        max(price) as max_price\n",
    "    from df\n",
    "    group by state\n",
    "    order by count(*) desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+---------+---------+\n",
      "|                city|  n|avg_price|max_price|\n",
      "+--------------------+---+---------+---------+\n",
      "|          Palm Beach| 68|   491.28|   1500.0|\n",
      "|              Malibu|337|   377.53|   4500.0|\n",
      "|   Pacific Palisades| 36|   326.00|    850.0|\n",
      "|         Watsonville| 80|   319.70|    782.0|\n",
      "|       Darling Point| 65|   309.03|   2001.0|\n",
      "|       Bilgola Beach| 32|   300.44|    890.0|\n",
      "|        Avalon Beach| 88|   278.93|   1000.0|\n",
      "|              Avalon| 82|   270.15|    850.0|\n",
      "|             Del Mar| 40|   266.20|    900.0|\n",
      "|            Tamarama|153|   258.26|   1000.0|\n",
      "|       Playa Del Rey| 34|   255.76|    599.0|\n",
      "|            La Jolla|124|   254.70|   2400.0|\n",
      "| Rancho Palos Verdes| 85|   253.44|   1250.0|\n",
      "|     Manhattan Beach|249|   252.19|   1000.0|\n",
      "|La CañAda Flintridge| 32|   250.88|    900.0|\n",
      "| Sydney Olympic Park| 40|   250.55|    520.0|\n",
      "|              Mosman|239|   246.82|   3701.0|\n",
      "|            Capitola| 72|   246.50|    650.0|\n",
      "|          Birchgrove| 35|   240.17|   1000.0|\n",
      "|             Newport|120|   237.67|    901.0|\n",
      "+--------------------+---+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Most expensive popular cities (original dataset)\n",
    "dataset.registerTempTable(\"df\")\n",
    "\n",
    "spark.sqlContext.sql(f\"\"\"\n",
    "    select \n",
    "        city,\n",
    "        count(*) as n,\n",
    "        cast(avg(price) as decimal(12,2)) as avg_price,\n",
    "        max(price) as max_price\n",
    "    from df\n",
    "    group by city\n",
    "    order by avg(price) desc\n",
    "\"\"\").filter(\"n>25\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define continous and categorical features and filter nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Step 2. Create our feature pipeline and train it on the entire dataset\n",
    "val continuousFeatures = Array(\"bathrooms\",\n",
    "  \"bedrooms\",\n",
    "  \"security_deposit\",\n",
    "  \"cleaning_fee\",\n",
    "  \"extra_people\",\n",
    "  \"number_of_reviews\",\n",
    "  \"square_feet\",\n",
    "  \"review_scores_rating\")\n",
    "\n",
    "val categoricalFeatures = Array(\"room_type\",\n",
    "  \"host_is_superhost\",\n",
    "  \"cancellation_policy\",\n",
    "  \"instant_bookable\",\n",
    "  \"state\")\n",
    "\n",
    "val allFeatures = continuousFeatures.union(categoricalFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321588\n"
     ]
    }
   ],
   "source": [
    "// Filter all null values\n",
    "val allCols = allFeatures.union(Seq(\"price\")).map(datasetImputed.col)\n",
    "val nullFilter = allCols.map(_.isNotNull).reduce(_ && _)\n",
    "val datasetImputedFiltered = datasetImputed.select(allCols: _*).filter(nullFilter).persist()\n",
    "\n",
    "println(datasetImputedFiltered.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Split data into training and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val Array(trainingDataset, validationDataset) = datasetImputedFiltered.randomSplit(Array(0.7, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Continous Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val continuousFeatureAssembler = new VectorAssembler(uid = \"continuous_feature_assembler\").\n",
    "    setInputCols(continuousFeatures).\n",
    "    setOutputCol(\"unscaled_continuous_features\")\n",
    "\n",
    "val continuousFeatureScaler = new StandardScaler(uid = \"continuous_feature_scaler\").\n",
    "    setInputCol(\"unscaled_continuous_features\").\n",
    "    setOutputCol(\"scaled_continuous_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Categorical Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val categoricalFeatureIndexers = categoricalFeatures.map {\n",
    "    feature => new StringIndexer(uid = s\"string_indexer_$feature\").\n",
    "      setInputCol(feature).\n",
    "      setOutputCol(s\"${feature}_index\")\n",
    "}\n",
    "val categoricalFeatureOneHotEncoders = categoricalFeatureIndexers.map {\n",
    "    indexer => new OneHotEncoder(uid = s\"oh_encoder_${indexer.getOutputCol}\").\n",
    "      setInputCol(indexer.getOutputCol).\n",
    "      setOutputCol(s\"${indexer.getOutputCol}_oh\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Assemble our features and feature pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have slightly different feature pipelines for LR and RF. This is done purely for demonstration purposes, whereas your actual models should scale continuous features for the RF model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished constructing the pipeline\n"
     ]
    }
   ],
   "source": [
    "val featureColsRf = categoricalFeatureIndexers.map(_.getOutputCol).union(Seq(\"scaled_continuous_features\"))\n",
    "val featureColsLr = categoricalFeatureOneHotEncoders.map(_.getOutputCol).union(Seq(\"scaled_continuous_features\"))\n",
    "\n",
    "// assemble all processes categorical and continuous features into a single feature vector\n",
    "val featureAssemblerLr = new VectorAssembler(uid = \"feature_assembler_lr\").\n",
    "    setInputCols(featureColsLr).\n",
    "    setOutputCol(\"features_lr\")\n",
    "val featureAssemblerRf = new VectorAssembler(uid = \"feature_assembler_rf\").\n",
    "    setInputCols(featureColsRf).\n",
    "    setOutputCol(\"features_rf\")\n",
    "\n",
    "val estimators: Array[PipelineStage] = Array(continuousFeatureAssembler, continuousFeatureScaler).\n",
    "    union(categoricalFeatureIndexers).\n",
    "    union(categoricalFeatureOneHotEncoders).\n",
    "    union(Seq(featureAssemblerLr, featureAssemblerRf))\n",
    "\n",
    "val featurePipeline = new Pipeline(uid = \"feature_pipeline\").\n",
    "    setStages(estimators)\n",
    "\n",
    "val sparkFeaturePipelineModel = featurePipeline.fit(datasetImputedFiltered)\n",
    "\n",
    "println(\"Finished constructing the pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete: Training Random Forest\n"
     ]
    }
   ],
   "source": [
    "// Step 3.1 Create our random forest model\n",
    "val randomForest = new RandomForestRegressor(uid = \"random_forest_regression\").\n",
    "    setFeaturesCol(\"features_rf\").\n",
    "    setLabelCol(\"price\").\n",
    "    setPredictionCol(\"price_prediction\")\n",
    "\n",
    "val sparkPipelineEstimatorRf = new Pipeline().setStages(Array(sparkFeaturePipelineModel, randomForest))\n",
    "val sparkPipelineRf = sparkPipelineEstimatorRf.fit(datasetImputedFiltered)\n",
    "\n",
    "println(\"Complete: Training Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Train Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete: Training Linear Regression\n"
     ]
    }
   ],
   "source": [
    "// Step 3.2 Create our linear regression model\n",
    "val linearRegression = new LinearRegression(uid = \"linear_regression\").\n",
    "    setFeaturesCol(\"features_lr\").\n",
    "    setLabelCol(\"price\").\n",
    "    setPredictionCol(\"price_prediction\")\n",
    "\n",
    "val sparkPipelineEstimatorLr = new Pipeline().setStages(Array(sparkFeaturePipelineModel, linearRegression))\n",
    "val sparkPipelineLr = sparkPipelineEstimatorLr.fit(datasetImputedFiltered)\n",
    "\n",
    "println(\"Complete: Training Linear Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9 (Optional): Serialize your models to bundle.ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkPipelineLr.serializeToBundle(new java.io.File(\"/tmp/model.lr\"))\n",
    "sparkPipelineRf.serializeToBundle(new java.io.File(\"/tmp/model.rf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10 (Optional): Deserialize your models from bundle.ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ml.combust.mleap.runtime.MleapContext.defaultContext\n",
    "import ml.combust.mleap.runtime.MleapSupport._\n",
    "import java.io.File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val (bundleLr, mleapTransformerLr) = new File(\"/tmp/model.lr\").deserializeBundle()\n",
    "val (bundleRf, mleapTransformerRf) = new File(\"/tmp/model.rf\").deserializeBundle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11 (Optional): Manually Create the LeapFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ml.combust.mleap.runtime._\n",
    "import ml.combust.mleap.runtime.types._\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val schema = StructType(Seq(StructField(\"features\", TensorType.doubleVector()))).get\n",
    "val dataset = LocalDataset(Seq(Row(Vectors.dense(Array(20.0, 10.0, 5.0)))))\n",
    "val frame = LeapFrame(schema, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12 (Optional): Deserialize and Score a LeapFrame from Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ml.combust.mleap.runtime.serialization.FrameReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"schema\": {\n",
      "    \"fields\": [{\n",
      "      \"name\": \"state\",\n",
      "      \"type\": \"string\"\n",
      "    }, {\n",
      "      \"name\": \"bathrooms\",\n",
      "      \"type\": \"double\"\n",
      "    }, {\n",
      "      \"name\": \"square_feet\",\n",
      "      \"type\": \"double\"\n",
      "    }, {\n",
      "      \"name\": \"bedrooms\",\n",
      "      \"type\": \"double\"\n",
      "    }, {\n",
      "      \"name\": \"security_deposit\",\n",
      "      \"type\": \"double\"\n",
      "    }, {\n",
      "      \"name\": \"cleaning_fee\",\n",
      "      \"type\": \"double\"\n",
      "    }, {\n",
      "      \"name\": \"extra_people\",\n",
      "      \"type\": \"double\"\n",
      "    }, {\n",
      "      \"name\": \"number_of_reviews\",\n",
      "      \"type\": \"double\"\n",
      "    }, {\n",
      "      \"name\": \"review_scores_rating\",\n",
      "      \"type\": \"double\"\n",
      "    }, {\n",
      "      \"name\": \"room_type\",\n",
      "      \"type\": \"string\"\n",
      "    }, {\n",
      "      \"name\": \"host_is_superhost\",\n",
      "      \"type\": \"string\"\n",
      "    }, {\n",
      "      \"name\": \"cancellation_policy\",\n",
      "      \"type\": \"string\"\n",
      "    }, {\n",
      "      \"name\": \"instant_bookable\",\n",
      "      \"type\": \"string\"\n",
      "    }]\n",
      "  },\n",
      "  \"rows\": [[\"NY\", 2.0, 1250.0, 3.0, 50.0, 30.0, 2.0, 56.0, 90.0, \"Entire home/apt\", \"1.0\", \"strict\", \"1.0\"]]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val s = scala.io.Source.fromURL(\"https://s3-us-west-2.amazonaws.com/mleap-demo/frame.json\").mkString\n",
    "\n",
    "println(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val bytes = s.getBytes(\"UTF-8\")\n",
    "val frame = FrameReader(\"ml.combust.mleap.json\").fromBytes(bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price LR: 232.62463916675608\n",
      "Price RF: 219.09900439790786\n"
     ]
    }
   ],
   "source": [
    "val frameLr = mleapTransformerLr.transform(frame).get.select(\"price_prediction\").get\n",
    "\n",
    "val frameRf = mleapTransformerRf.transform(frame).get.select(\"price_prediction\").get\n",
    "\n",
    "println(\"Price LR: \" + frameLr.dataset(0).getDouble(0))\n",
    "println(\"Price RF: \" + frameRf.dataset(0).getDouble(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13 (Optional): Score a Spark Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ml.combust.mleap.spark.SparkSupport._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val inputFile = \"file:////tmp/airbnb.avro\"\n",
    "\n",
    "var dataset = spark.sqlContext.read.format(\"com.databricks.spark.avro\").\n",
    "  load(inputFile)\n",
    "\n",
    "var datasetFiltered = dataset.filter(\"price >= 50 AND price <= 750 and bathrooms > 0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|       square_feet|             price|          bedrooms|         bathrooms|     cleaning_fee|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|            321588|            321588|            321588|            321588|           321588|\n",
      "|   mean| 546.7441757777032|131.54961006007687|1.3352426085550455| 1.199068373198005|37.64188340360959|\n",
      "| stddev|363.39839582374066| 90.10912788720098|0.8466586601060778|0.4830590051262673|42.64237791484579|\n",
      "|    min|             104.0|              50.0|               0.0|               0.5|              0.0|\n",
      "|    max|           32292.0|             750.0|              10.0|               8.0|            700.0|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasetFiltered.registerTempTable(\"df\")\n",
    "\n",
    "val datasetImputed = spark.sqlContext.sql(f\"\"\"\n",
    "    select\n",
    "        id,\n",
    "        city,\n",
    "        case when state in('NY', 'CA', 'London', 'Berlin', 'TX' ,'IL', 'OR', 'DC', 'WA')\n",
    "            then state\n",
    "            else 'Other'\n",
    "        end as state,\n",
    "        space,\n",
    "        price,\n",
    "        bathrooms,\n",
    "        bedrooms,\n",
    "        room_type,\n",
    "        host_is_superhost,\n",
    "        cancellation_policy,\n",
    "        case when security_deposit is null\n",
    "            then 0.0\n",
    "            else security_deposit\n",
    "        end as security_deposit,\n",
    "        price_per_bedroom,\n",
    "        case when number_of_reviews is null\n",
    "            then 0.0\n",
    "            else number_of_reviews\n",
    "        end as number_of_reviews,\n",
    "        case when extra_people is null\n",
    "            then 0.0\n",
    "            else extra_people\n",
    "        end as extra_people,\n",
    "        instant_bookable,\n",
    "        case when cleaning_fee is null\n",
    "            then 0.0\n",
    "            else cleaning_fee\n",
    "        end as cleaning_fee,\n",
    "        case when review_scores_rating is null\n",
    "            then 80.0\n",
    "            else review_scores_rating\n",
    "        end as review_scores_rating,\n",
    "        case when square_feet is not null and square_feet > 100\n",
    "            then square_feet\n",
    "            when (square_feet is null or square_feet <=100) and (bedrooms is null or bedrooms = 0)\n",
    "            then 350.0\n",
    "            else 380 * bedrooms\n",
    "        end as square_feet\n",
    "    from df\n",
    "    where bedrooms is not null\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "datasetImputed.select(\"square_feet\", \"price\", \"bedrooms\", \"bathrooms\", \"cleaning_fee\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val allCols = allFeatures.union(Seq(\"price\")).map(datasetImputed.col)\n",
    "val nullFilter = allCols.map(_.isNotNull).reduce(_ && _)\n",
    "val datasetImputedFiltered = datasetImputed.select(allCols: _*).filter(nullFilter).persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sparkDataframe = mleapTransformerLr.sparkTransform(datasetImputedFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(id, city, state, space, price, bathrooms, bedrooms, room_type, host_is_superhost, cancellation_policy, security_deposit, price_per_bedroom, number_of_reviews, extra_people, instant_bookable, cleaning_fee, review_scores_rating, square_feet, unscaled_continuous_features, scaled_continuous_features, room_type_index, host_is_superhost_index, cancellation_policy_index, instant_bookable_index, state_index, room_type_index_oh, host_is_superhost_index_oh, cancellation_policy_index_oh, instant_bookable_index_oh, state_index_oh, features_lr, features_rf, price_prediction)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkDataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 13, localhost): scala.MatchError: 8.0 (of class java.math.BigDecimal)\n",
       "\tat ml.combust.mleap.core.feature.VectorAssemblerModel$$anonfun$apply$2.apply(VectorAssemblerModel.scala:32)\n",
       "\tat ml.combust.mleap.core.feature.VectorAssemblerModel$$anonfun$apply$2.apply(VectorAssemblerModel.scala:32)\n",
       "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
       "\tat ml.combust.mleap.core.feature.VectorAssemblerModel.apply(VectorAssemblerModel.scala:32)\n",
       "\tat ml.combust.mleap.runtime.transformer.feature.VectorAssembler$$anonfun$1.apply(VectorAssembler.scala:15)\n",
       "\tat ml.combust.mleap.runtime.transformer.feature.VectorAssembler$$anonfun$1.apply(VectorAssembler.scala:15)\n",
       "\tat org.apache.spark.sql.mleap.UserDefinedFunctionConverters$$anonfun$convertFunction$1.apply(UserDefinedFunctionConverters.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 13, localhost): scala.MatchError: 8.0 (of class java.math.BigDecimal)\n",
       "\tat ml.combust.mleap.core.feature.VectorAssemblerModel$$anonfun$apply$2.apply(VectorAssemblerModel.scala:32)\n",
       "\tat ml.combust.mleap.core.feature.VectorAssemblerModel$$anonfun$apply$2.apply(VectorAssemblerModel.scala:32)\n",
       "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
       "\tat ml.combust.mleap.core.feature.VectorAssemblerModel.apply(VectorAssemblerModel.scala:32)\n",
       "\tat ml.combust.mleap.runtime.transformer.feature.VectorAssembler$$anonfun$1.apply(VectorAssembler.scala:15)\n",
       "\tat ml.combust.mleap.runtime.transformer.feature.VectorAssembler$$anonfun$1.apply(VectorAssembler.scala:15)\n",
       "\tat org.apache.spark.sql.mleap.UserDefinedFunctionConverters$$anonfun$convertFunction$1.apply(UserDefinedFunctionConverters.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347)\n",
       "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:39)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2183)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n",
       "  at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)\n",
       "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2182)\n",
       "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2189)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1925)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1924)\n",
       "  at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2562)\n",
       "  at org.apache.spark.sql.Dataset.head(Dataset.scala:1924)\n",
       "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2139)\n",
       "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:239)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:526)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:486)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:495)\n",
       "  ... 54 elided\n",
       "Caused by: scala.MatchError: 8.0 (of class java.math.BigDecimal)\n",
       "  at ml.combust.mleap.core.feature.VectorAssemblerModel$$anonfun$apply$2.apply(VectorAssemblerModel.scala:32)\n",
       "  at ml.combust.mleap.core.feature.VectorAssemblerModel$$anonfun$apply$2.apply(VectorAssemblerModel.scala:32)\n",
       "  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
       "  at ml.combust.mleap.core.feature.VectorAssemblerModel.apply(VectorAssemblerModel.scala:32)\n",
       "  at ml.combust.mleap.runtime.transformer.feature.VectorAssembler$$anonfun$1.apply(VectorAssembler.scala:15)\n",
       "  at ml.combust.mleap.runtime.transformer.feature.VectorAssembler$$anonfun$1.apply(VectorAssembler.scala:15)\n",
       "  at org.apache.spark.sql.mleap.UserDefinedFunctionConverters$$anonfun$convertFunction$1.apply(UserDefinedFunctionConverters.scala:32)\n",
       "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n",
       "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n",
       "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:85)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkDataframe.select(\"bedrooms\", \"bathrooms\", \"price\", \"price_prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Load the libaries for .deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:25: error: object client is not a member of package ml.combust\n",
       "       import ml.combust.client.spark.SparkSupport._\n",
       "                         ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ml.combust.bundle.BundleRegistry\n",
    "import ml.combust.mleap.spark.SparkSupport._\n",
    "import ml.combust.client.spark.SparkSupport._\n",
    "import ml.combust.client.model.MultiModelClient\n",
    "import akka.actor.ActorSystem\n",
    "import akka.stream.ActorMaterializer\n",
    "import scala.concurrent.duration._\n",
    "import scala.concurrent.Await\n",
    "\n",
    "implicit val hr = BundleRegistry(\"spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Set up the ActorSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "implicit val system = ActorSystem(\"combust-client\")\n",
    "implicit val materializer = ActorMaterializer()\n",
    "implicit val ec = system.dispatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Define model servers to send your models to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future release of this notebook, we'll add direction on how to set-up your public cloud account.\n",
    "\n",
    "For now, send us an email at mikhail@combust.ml to get an access key or a docker image of the model server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val client = MultiModelClient.builder[String]().\n",
    "  withServer(\"model-server-1\", \"http://combust-model-server-4671c502.124d9c7a.svc.dockerapp.io:8889\").\n",
    "  withServer(\"model-server-2\", \"http://combust-model-server-9ac83b2d.801b9d51.svc.dockerapp.io:8890\").\n",
    "  build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13: Deploy your LR and RF pipelines to the model servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Await.result(client.deploy(sparkPipelineLr, \"airbnb_lr\").sequence, 10.seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Await.result(client.deploy(sparkPipelineRf, \"airbnb_rf\").sequence, 10.seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14 (Optional): Undeploy your models from the model server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Await.result(client.undeploy(\"airbnb_rf\").sequence, 10.seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Await.result(client.undeploy(\"airbnb_lr\").sequence, 10.seconds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
