{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLeap.deploy() Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set-up running Spark 2.0 (required for this demo) from a Jupyter notebook, follow these [instructions](https://github.com/combust-ml/mleap/wiki/Setting-up-a-Spark-2.0-notebook-with-MLeap-and-Toree).\n",
    "\n",
    "This demo will show you how to:\n",
    "1. Load the research dataset from s3\n",
    "2. Construct a feature transformer pipeline using commonly available transformers in Spark\n",
    "3. Deploy your model to a public model server hosted on the combust.ml cloud using .deploy()\n",
    "\n",
    "NOTE: To run the actual deploy step you have to either:\n",
    "1. Get a key from combust.ml - it's easy, just email us!\n",
    "2. Fire up the combust cloud server on your local machine - also easy, send us an email and we'll send you a docker image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for the demo was pulled together from individual cities' data found [here](http://insideairbnb.com/get-the-data.html). We've also gone ahead and pulled the individual datasets and relevant features into this [research dataset](https://s3-us-west-2.amazonaws.com/mleap-demo/datasources/airbnb.avro) stored as avro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Load libraries and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we've made it so that you have to download the [data](https://s3-us-west-2.amazonaws.com/mleap-demo/datasources/airbnb.avro) from s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// %AddDeps ml.combust.mleap mleap-spark_2.11 0.4.0 --transitive\n",
    "// %AddDeps com.databricks spark-avro_2.11 3.0.1\n",
    "\n",
    "// Import the rest of the required packages\n",
    "import org.apache.spark.ml.feature.OneHotEncoder\n",
    "import org.apache.spark.ml.feature.{StandardScaler, StringIndexer, VectorAssembler}\n",
    "import org.apache.spark.ml.regression.{RandomForestRegressor, LinearRegression}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineStage}\n",
    "\n",
    "// MLeap/Bundle.ML Serialization Libraries\n",
    "import ml.combust.mleap.spark.SparkSupport._\n",
    "import resource._\n",
    "import ml.combust.bundle.BundleFile\n",
    "import org.apache.spark.ml.bundle.SparkBundleContext\n",
    "\n",
    "// MLeap/Bundle.ML De-Serialization Libraries\n",
    "import ml.combust.mleap.runtime.MleapContext.defaultContext\n",
    "import ml.combust.mleap.runtime.MleapSupport._\n",
    "import java.io.File\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT!!! You may have to run this next block of code a few times to get it to work - this is due to another bug in Toree. For me, running it twice works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389255\n",
      "321588\n"
     ]
    }
   ],
   "source": [
    "// Step 0. Load our Airbnb dataset\n",
    "\n",
    "val inputFile = \"file:////tmp/airbnb.avro\"\n",
    "\n",
    "var dataset = spark.sqlContext.read.format(\"com.databricks.spark.avro\").\n",
    "  load(inputFile)\n",
    "\n",
    "var datasetFiltered = dataset.filter(\"price >= 50 AND price <= 750 and bathrooms > 0.0\")\n",
    "println(dataset.count())\n",
    "println(datasetFiltered.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Standardize the data for our demo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|       square_feet|             price|          bedrooms|         bathrooms|     cleaning_fee|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|            321588|            321588|            321588|            321588|           321588|\n",
      "|   mean| 546.7441757777032|131.54961006007687|1.3352426085550455| 1.199068373198005|37.64188340360959|\n",
      "| stddev|363.39839582374066| 90.10912788720098|0.8466586601060778|0.4830590051262673|42.64237791484579|\n",
      "|    min|             104.0|              50.0|               0.0|               0.5|              0.0|\n",
      "|    max|           32292.0|             750.0|              10.0|               8.0|            700.0|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasetFiltered.registerTempTable(\"df\")\n",
    "\n",
    "val datasetImputed = spark.sqlContext.sql(f\"\"\"\n",
    "    select\n",
    "        id,\n",
    "        city,\n",
    "        case when state in('NY', 'CA', 'London', 'Berlin', 'TX' ,'IL', 'OR', 'DC', 'WA')\n",
    "            then state\n",
    "            else 'Other'\n",
    "        end as state,\n",
    "        space,\n",
    "        price,\n",
    "        bathrooms,\n",
    "        bedrooms,\n",
    "        room_type,\n",
    "        host_is_superhost,\n",
    "        cancellation_policy,\n",
    "        case when security_deposit is null\n",
    "            then 0.0\n",
    "            else security_deposit\n",
    "        end as security_deposit,\n",
    "        price_per_bedroom,\n",
    "        case when number_of_reviews is null\n",
    "            then 0.0\n",
    "            else number_of_reviews\n",
    "        end as number_of_reviews,\n",
    "        case when extra_people is null\n",
    "            then 0.0\n",
    "            else extra_people\n",
    "        end as extra_people,\n",
    "        instant_bookable,\n",
    "        case when cleaning_fee is null\n",
    "            then 0.0\n",
    "            else cleaning_fee\n",
    "        end as cleaning_fee,\n",
    "        case when review_scores_rating is null\n",
    "            then 80.0\n",
    "            else review_scores_rating\n",
    "        end as review_scores_rating,\n",
    "        case when square_feet is not null and square_feet > 100\n",
    "            then square_feet\n",
    "            when (square_feet is null or square_feet <=100) and (bedrooms is null or bedrooms = 0)\n",
    "            then 350.0\n",
    "            else 380 * bedrooms\n",
    "        end as square_feet\n",
    "    from df\n",
    "    where bedrooms is not null\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "datasetImputed.select(\"square_feet\", \"price\", \"bedrooms\", \"bathrooms\", \"cleaning_fee\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Take a look at some summary statistics of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+---------+---------+\n",
      "|        state|    n|avg_price|max_price|\n",
      "+-------------+-----+---------+---------+\n",
      "|           NY|48362|   146.75|    750.0|\n",
      "|           CA|44716|   158.76|    750.0|\n",
      "|Île-de-France|40732|   107.74|    750.0|\n",
      "|       London|17532|   117.71|    750.0|\n",
      "|          NSW|14416|   167.96|    750.0|\n",
      "|       Berlin|13098|    81.01|    650.0|\n",
      "|Noord-Holland| 8890|   128.56|    750.0|\n",
      "|          VIC| 8636|   144.49|    750.0|\n",
      "|North Holland| 7636|   134.60|    700.0|\n",
      "|           IL| 7544|   141.85|    750.0|\n",
      "|           ON| 7186|   129.05|    750.0|\n",
      "|           TX| 6702|   196.59|    750.0|\n",
      "|           WA| 5858|   132.48|    750.0|\n",
      "|    Catalonia| 5748|   106.39|    720.0|\n",
      "|           BC| 5522|   133.14|    750.0|\n",
      "|           DC| 5476|   136.56|    720.0|\n",
      "|       Québec| 5116|   104.98|    700.0|\n",
      "|    Catalunya| 4570|    99.36|    675.0|\n",
      "|       Veneto| 4486|   131.71|    700.0|\n",
      "|           OR| 4330|   114.02|    700.0|\n",
      "+-------------+-----+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Most popular cities (original dataset)\n",
    "\n",
    "spark.sqlContext.sql(f\"\"\"\n",
    "    select \n",
    "        state,\n",
    "        count(*) as n,\n",
    "        cast(avg(price) as decimal(12,2)) as avg_price,\n",
    "        max(price) as max_price\n",
    "    from df\n",
    "    group by state\n",
    "    order by count(*) desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+---------+---------+\n",
      "|                city|  n|avg_price|max_price|\n",
      "+--------------------+---+---------+---------+\n",
      "|          Palm Beach| 68|   491.28|   1500.0|\n",
      "|              Malibu|337|   377.53|   4500.0|\n",
      "|   Pacific Palisades| 36|   326.00|    850.0|\n",
      "|         Watsonville| 80|   319.70|    782.0|\n",
      "|       Darling Point| 65|   309.03|   2001.0|\n",
      "|       Bilgola Beach| 32|   300.44|    890.0|\n",
      "|        Avalon Beach| 88|   278.93|   1000.0|\n",
      "|              Avalon| 82|   270.15|    850.0|\n",
      "|             Del Mar| 40|   266.20|    900.0|\n",
      "|            Tamarama|153|   258.26|   1000.0|\n",
      "|       Playa Del Rey| 34|   255.76|    599.0|\n",
      "|            La Jolla|124|   254.70|   2400.0|\n",
      "| Rancho Palos Verdes| 85|   253.44|   1250.0|\n",
      "|     Manhattan Beach|249|   252.19|   1000.0|\n",
      "|La CañAda Flintridge| 32|   250.88|    900.0|\n",
      "| Sydney Olympic Park| 40|   250.55|    520.0|\n",
      "|              Mosman|239|   246.82|   3701.0|\n",
      "|            Capitola| 72|   246.50|    650.0|\n",
      "|          Birchgrove| 35|   240.17|   1000.0|\n",
      "|             Newport|120|   237.67|    901.0|\n",
      "+--------------------+---+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Most expensive popular cities (original dataset)\n",
    "dataset.registerTempTable(\"df\")\n",
    "\n",
    "spark.sqlContext.sql(f\"\"\"\n",
    "    select \n",
    "        city,\n",
    "        count(*) as n,\n",
    "        cast(avg(price) as decimal(12,2)) as avg_price,\n",
    "        max(price) as max_price\n",
    "    from df\n",
    "    group by city\n",
    "    order by avg(price) desc\n",
    "\"\"\").filter(\"n>25\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define continous and categorical features and filter nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Step 2. Create our feature pipeline and train it on the entire dataset\n",
    "val continuousFeatures = Array(\"bathrooms\",\n",
    "  \"bedrooms\",\n",
    "  \"security_deposit\",\n",
    "  \"cleaning_fee\",\n",
    "  \"extra_people\",\n",
    "  \"number_of_reviews\",\n",
    "  \"square_feet\",\n",
    "  \"review_scores_rating\")\n",
    "\n",
    "val categoricalFeatures = Array(\"room_type\",\n",
    "  \"host_is_superhost\",\n",
    "  \"cancellation_policy\",\n",
    "  \"instant_bookable\",\n",
    "  \"state\")\n",
    "\n",
    "val allFeatures = continuousFeatures.union(categoricalFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321588\n"
     ]
    }
   ],
   "source": [
    "// Filter all null values\n",
    "val allCols = allFeatures.union(Seq(\"price\")).map(datasetImputed.col)\n",
    "val nullFilter = allCols.map(_.isNotNull).reduce(_ && _)\n",
    "val datasetImputedFiltered = datasetImputed.select(allCols: _*).filter(nullFilter).persist()\n",
    "\n",
    "println(datasetImputedFiltered.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Split data into training and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val Array(trainingDataset, validationDataset) = datasetImputedFiltered.randomSplit(Array(0.7, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Continous Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val continuousFeatureAssembler = new VectorAssembler(uid = \"continuous_feature_assembler\").\n",
    "    setInputCols(continuousFeatures).\n",
    "    setOutputCol(\"unscaled_continuous_features\")\n",
    "\n",
    "val continuousFeatureScaler = new StandardScaler(uid = \"continuous_feature_scaler\").\n",
    "    setInputCol(\"unscaled_continuous_features\").\n",
    "    setOutputCol(\"scaled_continuous_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Categorical Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val categoricalFeatureIndexers = categoricalFeatures.map {\n",
    "    feature => new StringIndexer(uid = s\"string_indexer_$feature\").\n",
    "      setInputCol(feature).\n",
    "      setOutputCol(s\"${feature}_index\")\n",
    "}\n",
    "val categoricalFeatureOneHotEncoders = categoricalFeatureIndexers.map {\n",
    "    indexer => new OneHotEncoder(uid = s\"oh_encoder_${indexer.getOutputCol}\").\n",
    "      setInputCol(indexer.getOutputCol).\n",
    "      setOutputCol(s\"${indexer.getOutputCol}_oh\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Assemble our features and feature pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have slightly different feature pipelines for LR and RF. This is done purely for demonstration purposes, whereas your actual models should scale continuous features for the RF model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished constructing the pipeline\n"
     ]
    }
   ],
   "source": [
    "val featureColsRf = categoricalFeatureIndexers.map(_.getOutputCol).union(Seq(\"scaled_continuous_features\"))\n",
    "val featureColsLr = categoricalFeatureOneHotEncoders.map(_.getOutputCol).union(Seq(\"scaled_continuous_features\"))\n",
    "\n",
    "// assemble all processes categorical and continuous features into a single feature vector\n",
    "val featureAssemblerLr = new VectorAssembler(uid = \"feature_assembler_lr\").\n",
    "    setInputCols(featureColsLr).\n",
    "    setOutputCol(\"features_lr\")\n",
    "val featureAssemblerRf = new VectorAssembler(uid = \"feature_assembler_rf\").\n",
    "    setInputCols(featureColsRf).\n",
    "    setOutputCol(\"features_rf\")\n",
    "\n",
    "val estimators: Array[PipelineStage] = Array(continuousFeatureAssembler, continuousFeatureScaler).\n",
    "    union(categoricalFeatureIndexers).\n",
    "    union(categoricalFeatureOneHotEncoders).\n",
    "    union(Seq(featureAssemblerLr, featureAssemblerRf))\n",
    "\n",
    "val featurePipeline = new Pipeline(uid = \"feature_pipeline\").\n",
    "    setStages(estimators)\n",
    "\n",
    "val sparkFeaturePipelineModel = featurePipeline.fit(datasetImputedFiltered)\n",
    "\n",
    "println(\"Finished constructing the pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete: Training Random Forest\n"
     ]
    }
   ],
   "source": [
    "// Create our random forest model\n",
    "val randomForest = new RandomForestRegressor(uid = \"random_forest_regression\").\n",
    "    setFeaturesCol(\"features_rf\").\n",
    "    setLabelCol(\"price\").\n",
    "    setPredictionCol(\"price_prediction\")\n",
    "\n",
    "val sparkPipelineEstimatorRf = new Pipeline().setStages(Array(sparkFeaturePipelineModel, randomForest))\n",
    "val sparkPipelineRf = sparkPipelineEstimatorRf.fit(datasetImputedFiltered)\n",
    "\n",
    "println(\"Complete: Training Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Train Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete: Training Linear Regression\n"
     ]
    }
   ],
   "source": [
    "// Create our linear regression model\n",
    "val linearRegression = new LinearRegression(uid = \"linear_regression\").\n",
    "    setFeaturesCol(\"features_lr\").\n",
    "    setLabelCol(\"price\").\n",
    "    setPredictionCol(\"price_prediction\")\n",
    "\n",
    "val sparkPipelineEstimatorLr = new Pipeline().setStages(Array(sparkFeaturePipelineModel, linearRegression))\n",
    "val sparkPipelineLr = sparkPipelineEstimatorLr.fit(datasetImputedFiltered)\n",
    "\n",
    "println(\"Complete: Training Linear Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9 (Optional): Serialize your models to bundle.ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sbc = SparkBundleContext().withDataset(sparkPipelineLr.transform(datasetImputedFiltered))\n",
    "for(bf <- managed(BundleFile(\"jar:file:/tmp/airbnb.model.lr.zip\"))) {\n",
    "        sparkPipelineLr.writeBundle.save(bf)(sbc).get\n",
    "      }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sbcRf = SparkBundleContext().withDataset(sparkPipelineRf.transform(datasetImputedFiltered))\n",
    "for(bf <- managed(BundleFile(\"jar:file:/tmp/airbnb.model.rf.zip\"))) {\n",
    "        sparkPipelineRf.writeBundle.save(bf)(sbcRf).get\n",
    "      }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10 (Optional): Deserialize your models from bundle.ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you no longer need the spark context to run this step #10 as well as #11, meaning you can score data with the model trained in spark, but without any dependency on spark (context) it self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val mleapTransformerLr = (for(bf <- managed(BundleFile(\"jar:file:/tmp/airbnb.model.lr.zip\"))) yield {\n",
    "      bf.loadMleapBundle().get.root\n",
    "    }).tried.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val mleapTransformerRf = (for(bf <- managed(BundleFile(\"jar:file:/tmp/airbnb.model.rf.zip\"))) yield {\n",
    "      bf.loadMleapBundle().get.root\n",
    "    }).tried.get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11 (Optional): Deserialize and Score a LeapFrame from Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ml.combust.mleap.runtime.serialization.FrameReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"schema\": {\n",
      "    \"fields\": [{\n",
      "      \"name\": \"state\",\n",
      "      \"type\": \"string\"\n",
      "    }, {\n",
      "      \"name\": \"bathrooms\",\n",
      "      \"type\": \"double\"\n",
      "    }, {\n",
      "      \"name\": \"square_feet\",\n",
      "      \"type\": \"double\"\n",
      "    }, {\n",
      "      \"name\": \"bedrooms\",\n",
      "      \"type\": \"double\"\n",
      "    }, {\n",
      "      \"name\": \"security_deposit\",\n",
      "      \"type\": \"double\"\n",
      "    }, {\n",
      "      \"name\": \"cleaning_fee\",\n",
      "      \"type\": \"double\"\n",
      "    }, {\n",
      "      \"name\": \"extra_people\",\n",
      "      \"type\": \"double\"\n",
      "    }, {\n",
      "      \"name\": \"number_of_reviews\",\n",
      "      \"type\": \"double\"\n",
      "    }, {\n",
      "      \"name\": \"review_scores_rating\",\n",
      "      \"type\": \"double\"\n",
      "    }, {\n",
      "      \"name\": \"room_type\",\n",
      "      \"type\": \"string\"\n",
      "    }, {\n",
      "      \"name\": \"host_is_superhost\",\n",
      "      \"type\": \"string\"\n",
      "    }, {\n",
      "      \"name\": \"cancellation_policy\",\n",
      "      \"type\": \"string\"\n",
      "    }, {\n",
      "      \"name\": \"instant_bookable\",\n",
      "      \"type\": \"string\"\n",
      "    }]\n",
      "  },\n",
      "  \"rows\": [[\"NY\", 2.0, 1250.0, 3.0, 50.0, 30.0, 2.0, 56.0, 90.0, \"Entire home/apt\", \"1.0\", \"strict\", \"1.0\"]]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val s = scala.io.Source.fromURL(\"https://s3-us-west-2.amazonaws.com/mleap-demo/frame.json\").mkString\n",
    "\n",
    "println(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price LR: 232.62463916675608\n",
      "Price RF: 219.09900439790786\n"
     ]
    }
   ],
   "source": [
    "val bytes = s.getBytes(\"UTF-8\")\n",
    "\n",
    "for(frame <- FrameReader(\"ml.combust.mleap.json\").fromBytes(bytes);\n",
    "    frameLr <- mleapTransformerLr.transform(frame);\n",
    "    frameLrSelect <- frameLr.select(\"price_prediction\");\n",
    "    frameRf <- mleapTransformerRf.transform(frame);\n",
    "    frameRfSelect <- frameRf.select(\"price_prediction\")) {\n",
    "      println(\"Price LR: \" + frameLrSelect.dataset(0).getDouble(0))\n",
    "      println(\"Price RF: \" + frameRfSelect.dataset(0).getDouble(0))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12 (Optional): Score a Spark Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ml.combust.mleap.spark.SparkSupport._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val inputFile = \"file:////tmp/airbnb.avro\"\n",
    "\n",
    "var dataset = spark.sqlContext.read.format(\"com.databricks.spark.avro\").\n",
    "  load(inputFile)\n",
    "\n",
    "var datasetFiltered = dataset.filter(\"price >= 50 AND price <= 750 and bathrooms > 0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val continuousFeatures = Array(\"bathrooms\",\n",
    "  \"bedrooms\",\n",
    "  \"security_deposit\",\n",
    "  \"cleaning_fee\",\n",
    "  \"extra_people\",\n",
    "  \"number_of_reviews\",\n",
    "  \"square_feet\",\n",
    "  \"review_scores_rating\")\n",
    "\n",
    "val categoricalFeatures = Array(\"room_type\",\n",
    "  \"host_is_superhost\",\n",
    "  \"cancellation_policy\",\n",
    "  \"instant_bookable\",\n",
    "  \"state\")\n",
    "\n",
    "val allFeatures = continuousFeatures.union(categoricalFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|       square_feet|             price|          bedrooms|         bathrooms|     cleaning_fee|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|            321588|            321588|            321588|            321588|           321588|\n",
      "|   mean| 546.7441757777032|131.54961006007687|1.3352426085550455| 1.199068373198005|37.64188340360959|\n",
      "| stddev|363.39839582374066| 90.10912788720098|0.8466586601060778|0.4830590051262673|42.64237791484579|\n",
      "|    min|             104.0|              50.0|               0.0|               0.5|              0.0|\n",
      "|    max|           32292.0|             750.0|              10.0|               8.0|            700.0|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasetFiltered.registerTempTable(\"df\")\n",
    "\n",
    "val datasetImputed = spark.sqlContext.sql(f\"\"\"\n",
    "    select\n",
    "        id,\n",
    "        city,\n",
    "        case when state in('NY', 'CA', 'London', 'Berlin', 'TX' ,'IL', 'OR', 'DC', 'WA')\n",
    "            then state\n",
    "            else 'Other'\n",
    "        end as state,\n",
    "        space,\n",
    "        price,\n",
    "        bathrooms,\n",
    "        bedrooms,\n",
    "        room_type,\n",
    "        host_is_superhost,\n",
    "        cancellation_policy,\n",
    "        case when security_deposit is null\n",
    "            then 0.0\n",
    "            else security_deposit\n",
    "        end as security_deposit,\n",
    "        price_per_bedroom,\n",
    "        case when number_of_reviews is null\n",
    "            then 0.0\n",
    "            else number_of_reviews\n",
    "        end as number_of_reviews,\n",
    "        case when extra_people is null\n",
    "            then 0.0\n",
    "            else extra_people\n",
    "        end as extra_people,\n",
    "        instant_bookable,\n",
    "        case when cleaning_fee is null\n",
    "            then 0.0\n",
    "            else cleaning_fee\n",
    "        end as cleaning_fee,\n",
    "        case when review_scores_rating is null\n",
    "            then 80.0\n",
    "            else review_scores_rating\n",
    "        end as review_scores_rating,\n",
    "        case when square_feet is not null and square_feet > 100\n",
    "            then square_feet\n",
    "            when (square_feet is null or square_feet <=100) and (bedrooms is null or bedrooms = 0)\n",
    "            then 350.0\n",
    "            else 380 * bedrooms\n",
    "        end as square_feet\n",
    "    from df\n",
    "    where bedrooms is not null\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "datasetImputed.select(\"square_feet\", \"price\", \"bedrooms\", \"bathrooms\", \"cleaning_fee\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val allCols = allFeatures.union(Seq(\"price\")).map(datasetImputed.col)\n",
    "val nullFilter = allCols.map(_.isNotNull).reduce(_ && _)\n",
    "val datasetImputedFiltered = datasetImputed.select(allCols: _*).filter(nullFilter).persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sparkDataframe = mleapTransformerLr.sparkTransform(datasetImputedFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(bathrooms, bedrooms, security_deposit, cleaning_fee, extra_people, number_of_reviews, square_feet, review_scores_rating, room_type, host_is_superhost, cancellation_policy, instant_bookable, state, price, unscaled_continuous_features, scaled_continuous_features, room_type_index, host_is_superhost_index, cancellation_policy_index, instant_bookable_index, state_index, room_type_index_oh, host_is_superhost_index_oh, cancellation_policy_index_oh, instant_bookable_index_oh, state_index_oh, features_lr, features_rf, price_prediction)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkDataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----+------------------+\n",
      "|bedrooms|bathrooms|price|  price_prediction|\n",
      "+--------+---------+-----+------------------+\n",
      "|     1.0|      1.0| 80.0|102.08902238557688|\n",
      "|     1.0|      1.5|200.0| 74.55365489475544|\n",
      "|     1.0|      1.0| 75.0|  59.3141121583095|\n",
      "|     1.0|      1.0| 70.0| 76.58053858965111|\n",
      "|     1.0|      1.0| 80.0|102.08902238557688|\n",
      "|     1.0|      1.5|200.0| 74.55365489475544|\n",
      "|     1.0|      1.0| 75.0|  59.3141121583095|\n",
      "|     1.0|      1.0| 70.0|  70.7874939184863|\n",
      "|     1.0|      1.0|110.0|110.28303665012884|\n",
      "|     2.0|      1.0| 91.0|125.17538136820143|\n",
      "+--------+---------+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkDataframe.select(\"bedrooms\", \"bathrooms\", \"price\", \"price_prediction\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13: Set up the ActorSystem (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Libraries to deploy models to combust cloud (optional)\n",
    "import akka.actor.ActorSystem\n",
    "import akka.stream.ActorMaterializer\n",
    "import scala.concurrent.duration._\n",
    "import scala.concurrent.Await\n",
    "implicit val system = ActorSystem(\"combust-client\")\n",
    "implicit val materializer = ActorMaterializer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14: Define model servers to send your models to and .deploy() (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future release of this notebook, we'll add direction on how to set-up your public cloud account.\n",
    "\n",
    "For now, send us an email at mikhail@combust.ml to get an access key or a docker image of the model server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadModelResponse(ResourceAlias(ResourceKey(my_username,my_model_rf,models),Some(Resource(4f3b9a5c-93f7-47d4-ab3d-e1e1d646d529,models))))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// {\n",
    "//     implicit val context = sbc\n",
    "//     Await.result(sparkPipelineLr.deploy(\"http://models.combust.ml:65327\", \"my_username\", \"my_model_lr\", token), 10.seconds)\n",
    "// }\n",
    "// {\n",
    "//     implicit val context = sbcRf\n",
    "//     Await.result(sparkPipelineRf.deploy(\"http://models.combust.ml:65327\", \"my_username\", \"my_model_rf\", token), 10.seconds)\n",
    "// }\n",
    "\n",
    "\n",
    "{\n",
    "    implicit val context = sbc\n",
    "    Await.result(sparkPipelineLr.deploy(\"http://localhost:65327\", \"my_username\", \"my_model_lr\"), 10.seconds)\n",
    "}\n",
    "{\n",
    "    implicit val context = sbcRf\n",
    "    Await.result(sparkPipelineRf.deploy(\"http://localhost:65327\", \"my_username\", \"my_model_rf\"), 10.seconds)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combust Cloud Client API (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ml.combust.model.core.domain.v1._\n",
    "import scala.concurrent.duration._\n",
    "import scala.concurrent.Await\n",
    "import ml.combust.mleap.runtime.serialization.FrameReader\n",
    "val client = ml.combust.model.client.Client(\"http://localhost:65327\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DefaultLeapFrame(StructType(List(StructField(state,StringType(false)), StructField(bathrooms,DoubleType(false)), StructField(square_feet,DoubleType(false)), StructField(bedrooms,DoubleType(false)), StructField(security_deposit,DoubleType(false)), StructField(cleaning_fee,DoubleType(false)), StructField(extra_people,DoubleType(false)), StructField(number_of_reviews,DoubleType(false)), StructField(review_scores_rating,DoubleType(false)), StructField(room_type,StringType(false)), StructField(host_is_superhost,StringType(false)), StructField(cancellation_policy,StringType(false)), StructField(instant_bookable,StringType(false)), StructField(unscaled_continuous_features,TensorType(DoubleType(false),false)), StructField(scaled_continuous_fea..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val s = scala.io.Source.fromURL(\"https://s3-us-west-2.amazonaws.com/mleap-demo/frame.json\").mkString\n",
    "val bytes = s.getBytes(\"UTF-8\")\n",
    "\n",
    "var leapFrame = FrameReader(\"ml.combust.mleap.json\").fromBytes(bytes).get\n",
    "val result = Await.result(client.transform(TransformRequest().withUsername(\"my_username\").withModelId(\"my_model_lr\").withFrame(leapFrame)), 10.seconds)\n",
    "val leapFrameFromServer = result.frame\n",
    "leapFrameFromServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232.62463916675608"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leapFrameFromServer.select(\"price_prediction\").get.dataset(0).getDouble(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala2",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
