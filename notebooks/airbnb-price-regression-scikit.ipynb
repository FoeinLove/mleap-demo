{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLeap Scikit-Learn Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this demo is to:\n",
    "    1. Put together an ML pipeline using scikit transformers, pipeline and feature unions\n",
    "    2. Train a linear regression to predict listing prices\n",
    "    3. Demonstrate how to serialize scikit-learn transformers and models to bundle.ml\n",
    "    4. TODO: use .deploy() to deploy a model to combust cloud\n",
    "    5. TODO: deserialize the pipeline in Spark\n",
    "    \n",
    "Note: MLeap <> Scikit-Learn itegration is experimental. We are planning to release a stable version with mleap-0.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for the demo was pulled together from individual cities' data found [here](http://insideairbnb.com/get-the-data.html). We've also gone ahead and pulled the individual datasets and relevant features into this [research dataset](https://s3-us-west-2.amazonaws.com/mleap-demo/datasources/airbnb.avro.zip) stored as avro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "# Make sure to checkout the feature/scikit-v2 branch\n",
    "sys.path.append('/Users/mikhail/combust/combust-mleap/python')\n",
    "\n",
    "import mleap.sklearn.pipeline\n",
    "import mleap.sklearn.feature_union\n",
    "import mleap.sklearn.base\n",
    "import mleap.sklearn.logistic\n",
    "import mleap.sklearn.preprocessing.data\n",
    "from mleap.sklearn.ensemble import forest\n",
    "\n",
    "from mleap.sklearn.preprocessing.data import FeatureExtractor, NDArrayToDataFrame, ToDense\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble.forest import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/mikhail/combust/mleap-demo/data/airbnb/airbnb.csv', error_bad_lines=False, warn_bad_lines=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Standardize the data for out demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _transform_state(state):\n",
    "    if state in ['NY', 'CA', 'London', 'Berlin', 'TX', 'IL', 'OR', 'DC', 'WA']:\n",
    "        return state\n",
    "    return 'Other'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Take a look at some summary statistics of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[['state', 'price']].groupby('state').agg([np.size, np.mean]).sort_values(by=('price', 'size'), ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "price_stats=df[['state', 'price']].groupby('state').agg([np.size, np.mean, np.max]).sort_values(by=('price', 'mean'), ascending=False)\n",
    "price_stats[price_stats[('price','size')]>25][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert to categorical feature\n",
    "df['host_is_superhost'] = df['host_is_superhost'].apply(str)\n",
    "df['instant_bookable'] = df['instant_bookable'].apply(str)\n",
    "\n",
    "# normalize state\n",
    "df['state'] = df.state.apply(_transform_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define continous and categorical features and filter nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "continuous_features = [\"bathrooms\",\n",
    "  \"bedrooms\",\n",
    "  \"security_deposit\",\n",
    "  \"cleaning_fee\",\n",
    "  \"extra_people\",\n",
    "  \"number_of_reviews\",\n",
    "  \"square_feet\",\n",
    "  \"review_scores_rating\"]\n",
    "\n",
    "categorical_features = [\"room_type\",\n",
    "  \"host_is_superhost\",\n",
    "  \"cancellation_policy\",\n",
    "  \"state\",\n",
    "  \"instant_bookable\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imputed_continuous_features = ['imp_{}'.format(x) for x in continuous_features]\n",
    "\n",
    "feature_extractor2_tf = FeatureExtractor(continuous_features, 'imputed_features', imputed_continuous_features)\n",
    "\n",
    "impute_security_deposit_tf = Imputer(strategy='mean', axis=0)\n",
    "impute_security_deposit_tf.minit(input_features=feature_extractor2_tf.output_vector, output_features='imputed_features')\n",
    "\n",
    "impute_pipeline = Pipeline([\n",
    "        (feature_extractor2_tf.name, feature_extractor2_tf),\n",
    "        (impute_security_deposit_tf.name, impute_security_deposit_tf)\n",
    "    ])\n",
    "impute_pipeline.minit()\n",
    "\n",
    "# Consider doing this via a feature union\n",
    "df2 = df.join(pd.DataFrame(impute_pipeline.fit_transform(df), columns=feature_extractor2_tf.output_vector_items))\n",
    "\n",
    "all_features = imputed_continuous_features + categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Split data into training and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First filter out outlier prices\n",
    "df2 = df2[(df2.price>=50)&(df2.price<=500)]\n",
    "\n",
    "# Split into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df2[all_features], df2[['price']], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Continous Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_extractor_tf = FeatureExtractor(imputed_continuous_features, 'unscaled_cont_features', [\"scaled_{}\".format(x) for x in imputed_continuous_features])\n",
    "\n",
    "standard_scaler_tf = StandardScaler()\n",
    "standard_scaler_tf.minit(input_features=feature_extractor_tf.output_vector, output_features='scaled_cont_features')\n",
    "\n",
    "standard_scaler_pipeline = Pipeline([(feature_extractor_tf.name, feature_extractor_tf),\n",
    "                            (standard_scaler_tf.name, standard_scaler_tf)])\n",
    "standard_scaler_pipeline.minit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Categorical Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Need to fix scikit's One-Hot-Encoder to drop the last column of a matrix if we're using it for ML\n",
    "def _create_le_one_hot_pipeline(feature_name):\n",
    "    feature_extractor3_tf = FeatureExtractor([feature_name], '{}_label'.format(feature_name), \n",
    "                                         ['{}_label_encoded'.format(feature_name)])\n",
    "\n",
    "    # Label Encoder for x1 Label \n",
    "    label_encoder_tf = LabelEncoder()\n",
    "    label_encoder_tf.minit(input_features = feature_extractor3_tf.output_vector, output_features='{}_label_le'.format(feature_name))\n",
    "\n",
    "    # Convert output of Label Encoder to Data Frame instead of 1d-array\n",
    "    n_dim_array_to_df_tf = NDArrayToDataFrame(feature_extractor3_tf.output_vector_items)\n",
    "\n",
    "    # Vector Assembler for x1 One Hot Encoder\n",
    "    one_hot_encoder_tf = OneHotEncoder()\n",
    "    one_hot_encoder_tf.minit(input_features = label_encoder_tf.output_features, output_features = '{}_label_one_hot_encoded'.format(feature_name))\n",
    "\n",
    "    #To Dense\n",
    "    to_dense_tf = ToDense(one_hot_encoder_tf.output_features)\n",
    "\n",
    "    one_hot_encoder_pipeline_x0 = Pipeline([\n",
    "                                             (feature_extractor3_tf.name, feature_extractor3_tf),\n",
    "                                             (label_encoder_tf.name, label_encoder_tf),\n",
    "                                             (n_dim_array_to_df_tf.name, n_dim_array_to_df_tf),\n",
    "                                             (one_hot_encoder_tf.name, one_hot_encoder_tf),\n",
    "                                             (to_dense_tf.name, to_dense_tf)\n",
    "                                            ])\n",
    "    \n",
    "    one_hot_encoder_pipeline_x0.minit()\n",
    "    \n",
    "    return one_hot_encoder_pipeline_x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oh_pipelines = [_create_le_one_hot_pipeline(x) for x in categorical_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Assemble our features and feature pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_union = FeatureUnion([\n",
    "        (standard_scaler_pipeline.name, standard_scaler_pipeline)\n",
    "    ] + [(x.name, x) for x in oh_pipelines])\n",
    "feature_union.minit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "standard_scaler_pipeline.fit_transform(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Define our linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put all of the categorical features into a list\n",
    "oh_features_lists = [[y[1].output_features for y in x.steps if y[1].op == 'one_hot_encoder'] for x in oh_pipelines]\n",
    "oh_features = [item for sublist in oh_features_lists for item in sublist]\n",
    "oh_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vector Assembler, for serialization purposes only\n",
    "feature_extractor_lr_model_tf = FeatureExtractor([standard_scaler_tf.output_features] + oh_features, 'input_features', [standard_scaler_tf.output_features] + oh_features)\n",
    "feature_extractor_lr_model_tf.skip_fit_transform = True\n",
    "\n",
    "# Define our linear regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.minit(input_features='input_features', prediction_column='price_prediction')\n",
    "\n",
    "lr_model_pipeline = Pipeline([\n",
    "        (feature_extractor_lr_model_tf.name, feature_extractor_lr_model_tf),\n",
    "        (lr_model.name, lr_model)\n",
    "    ])\n",
    "lr_model_pipeline.minit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline([(feature_union.name, feature_union),\n",
    "                            (lr_model_pipeline.name, lr_model_pipeline)])\n",
    "\n",
    "model_pipeline.minit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Define our Random Forest Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vector Assembler, for serialization purposes only\n",
    "feature_extractor_rf_model_tf = FeatureExtractor(imputed_continuous_features, 'input_features', imputed_continuous_features)\n",
    "feature_extractor_rf_model_tf.skip_fit_transform = True\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor(max_depth=4, n_estimators=11)\n",
    "rf.minit(input_features=feature_extractor_rf_model_tf.output_vector, prediction_column='price_prediction', feature_names=imputed_continuous_features)\n",
    "\n",
    "rf_model_pipeline = Pipeline([\n",
    "        (feature_extractor_rf_model_tf.name, feature_extractor_rf_model_tf),\n",
    "        (rf.name, rf)\n",
    "    ])\n",
    "rf_model_pipeline.minit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_model_pipeline.fit(X_train[imputed_continuous_features], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Fit our pipeline and regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "params = {\n",
    "    \"{}__max_depth\".format(rf.name): [5, 10],\n",
    "    \"{}__n_estimators\".format(rf.name): [10, 15, 20]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(estimator=rf_model_pipeline, param_grid=params, n_jobs=-1)\n",
    "rf_grid.fit(X_train[imputed_continuous_features], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_rf = rf_grid.best_params_\n",
    "best_max_depth = best_rf[\"{}__max_depth\".format(rf.name)]\n",
    "best_n_estimators = best_rf[\"{}__n_estimators\".format(rf.name)]\n",
    "\n",
    "rf = RandomForestRegressor(max_depth=best_max_depth, n_estimators=best_n_estimators)\n",
    "rf.minit(input_features=feature_extractor_rf_model_tf.output_vector, prediction_column='price_prediction', feature_names=imputed_continuous_features)\n",
    "\n",
    "rf_model_pipeline = Pipeline([\n",
    "        (feature_extractor_rf_model_tf.name, feature_extractor_rf_model_tf),\n",
    "        (rf.name, rf)\n",
    "    ])\n",
    "rf_model_pipeline.minit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_pipeline.fit(X_train, y_train)\n",
    "rf_model_pipeline.fit(X_train[imputed_continuous_features], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Serialize our pipelines to bundle.ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_pipeline.serialize_to_bundle('/tmp', 'scikit-airbnb.lr', init=True)\n",
    "rf_model_pipeline.serialize_to_bundle('/tmp', 'scikit-airbnb.rf', init=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
