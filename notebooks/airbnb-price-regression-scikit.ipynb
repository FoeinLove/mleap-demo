{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLeap Scikit-Learn Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this demo is to:\n",
    "    1. Put together an ML pipeline using scikit transformers, pipeline and feature unions\n",
    "    2. Train a linear regression to predict listing prices\n",
    "    3. Demonstrate how to serialize scikit-learn transformers and models to bundle.ml\n",
    "    4. TODO: use .deploy() to deploy a model to combust cloud\n",
    "    5. TODO: deserialize the pipeline in Spark\n",
    "    \n",
    "Note: MLeap <> Scikit-Learn itegration is experimental. We are planning to release a stable version with mleap-0.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for the demo was pulled together from individual cities' data found [here](http://insideairbnb.com/get-the-data.html). We've also gone ahead and pulled the individual datasets and relevant features into this [research dataset](https://s3-us-west-2.amazonaws.com/mleap-demo/datasources/airbnb.csv) stored as csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import mleap.sklearn.pipeline\n",
    "import mleap.sklearn.feature_union\n",
    "import mleap.sklearn.base\n",
    "import mleap.sklearn.logistic\n",
    "import mleap.sklearn.preprocessing.data\n",
    "from mleap.sklearn.ensemble import forest\n",
    "\n",
    "from mleap.sklearn.preprocessing.data import FeatureExtractor, NDArrayToDataFrame, ToDense, ReshapeArrayToN1, LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble.forest import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/mikhail/combust/mleap-demo/data/airbnb/airbnb.csv', error_bad_lines=False, warn_bad_lines=False)\n",
    "\n",
    "\n",
    "\n",
    "continuous_features = [\"bathrooms\",\n",
    "  \"bedrooms\",\n",
    "  \"security_deposit\",\n",
    "  \"cleaning_fee\",\n",
    "  \"extra_people\",\n",
    "  \"number_of_reviews\",\n",
    "  \"square_feet\",\n",
    "  \"review_scores_rating\"]\n",
    "\n",
    "df[continuous_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _transform_state(state):\n",
    "    if state in ['NY', 'CA', 'London', 'Berlin', 'TX', 'IL', 'OR', 'DC', 'WA']:\n",
    "        return state\n",
    "    return 'Other'\n",
    "\n",
    "continuous_features = [\"bathrooms\",\n",
    "  \"bedrooms\",\n",
    "  \"security_deposit\",\n",
    "  \"cleaning_fee\",\n",
    "  \"extra_people\",\n",
    "  \"number_of_reviews\",\n",
    "  \"square_feet\",\n",
    "  \"review_scores_rating\"]\n",
    "\n",
    "categorical_features = [\"room_type\",\n",
    "  \"host_is_superhost\",\n",
    "  \"cancellation_policy\",\n",
    "  \"state\",\n",
    "  \"instant_bookable\"]\n",
    "\n",
    "df['state'] = df.state.apply(_transform_state)\n",
    "\n",
    "imputed_continuous_features = ['imp_{}'.format(x) for x in continuous_features]\n",
    "\n",
    "feature_extractor2_tf = FeatureExtractor(input_scalars=continuous_features, \n",
    "                                         output_vector='imputed_features', \n",
    "                                         output_vector_items=imputed_continuous_features\n",
    "                                        )\n",
    "\n",
    "impute_security_deposit_tf = Imputer(strategy='mean', axis=0)\n",
    "impute_security_deposit_tf.mlinit(prior_tf=feature_extractor2_tf)\n",
    "\n",
    "impute_pipeline = Pipeline([\n",
    "        (feature_extractor2_tf.name, feature_extractor2_tf),\n",
    "        (impute_security_deposit_tf.name, impute_security_deposit_tf)\n",
    "    ])\n",
    "impute_pipeline.mlinit()\n",
    "\n",
    "# Consider doing this via a feature union\n",
    "df2 = df.join(pd.DataFrame(impute_pipeline.fit_transform(df), columns=feature_extractor2_tf.output_vector_items))\n",
    "\n",
    "all_features = imputed_continuous_features + categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "continuous_features = [\"bathrooms\",\n",
    "  \"bedrooms\",\n",
    "  \"security_deposit\",\n",
    "  \"cleaning_fee\",\n",
    "  \"extra_people\",\n",
    "  \"number_of_reviews\",\n",
    "  \"square_feet\",\n",
    "  \"review_scores_rating\"]\n",
    "imputed_continuous_features = ['imp_{}'.format(x) for x in continuous_features]\n",
    "\n",
    "feature_extractor2_tf = FeatureExtractor(input_scalars=continuous_features, \n",
    "                                         output_vector='imputed_features', \n",
    "                                         output_vector_items=imputed_continuous_features)\n",
    "\n",
    "impute_security_deposit_tf = Imputer(strategy='mean', axis=0)\n",
    "impute_security_deposit_tf.mlinit(prior_tf=feature_extractor2_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2[['state', 'imp_bathrooms', 'imp_square_feet', 'imp_bedrooms', 'imp_security_deposit', 'imp_cleaning_fee', 'imp_extra_people', 'imp_number_of_reviews', 'imp_review_scores_rating', 'room_type', 'host_is_superhost', 'cancellation_policy', 'instant_bookable', 'price']].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Standardize the data for out demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _transform_state(state):\n",
    "    if state in ['NY', 'CA', 'London', 'Berlin', 'TX', 'IL', 'OR', 'DC', 'WA']:\n",
    "        return state\n",
    "    return 'Other'\n",
    "\n",
    "def _set_min_of_one(x):\n",
    "    if x <= 1.0:\n",
    "        return 1.0\n",
    "    return x\n",
    "\n",
    "for x in ['imp_bathrooms', 'imp_square_feet', 'imp_bedrooms', 'imp_security_deposit', 'imp_cleaning_fee', 'imp_extra_people', 'imp_number_of_reviews', 'imp_review_scores_rating']:\n",
    "    df2[x] = df2[x].apply(_set_min_of_one)\n",
    "\n",
    "df2[(df2.price >= 50)&(df2.price <= 500)&(df2.imp_bathrooms>0)][['state', 'imp_bathrooms', 'imp_square_feet', 'imp_bedrooms', 'imp_security_deposit', 'imp_cleaning_fee', 'imp_extra_people', 'imp_number_of_reviews', 'imp_review_scores_rating', 'room_type', 'host_is_superhost', 'cancellation_policy', 'instant_bookable', 'price']][:250000].to_csv('/Users/mikhail/Documents/airbnb_for_stream_train.csv', index=False)\n",
    "\n",
    "df2[(df2.price >= 50)&(df2.price <= 500)&(df2.imp_bathrooms>0)][['state', 'imp_bathrooms', 'imp_square_feet', 'imp_bedrooms', 'imp_security_deposit', 'imp_cleaning_fee', 'imp_extra_people', 'imp_number_of_reviews', 'imp_review_scores_rating', 'room_type', 'host_is_superhost', 'cancellation_policy', 'instant_bookable', 'price']][150000:].to_csv('/Users/mikhail/Documents/airbnb_for_stream_train_update.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Take a look at some summary statistics of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[['state', 'price']].groupby('state').agg([np.size, np.mean]).sort_values(by=('price', 'size'), ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "price_stats=df[['state', 'price']].groupby('state').agg([np.size, np.mean, np.max]).sort_values(by=('price', 'mean'), ascending=False)\n",
    "price_stats[price_stats[('price','size')]>25][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to categorical feature\n",
    "df['host_is_superhost'] = df['host_is_superhost'].apply(str)\n",
    "df['instant_bookable'] = df['instant_bookable'].apply(str)\n",
    "\n",
    "# normalize state\n",
    "df['state'] = df.state.apply(_transform_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define continous and categorical features and filter nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "continuous_features = [\"bathrooms\",\n",
    "  \"bedrooms\",\n",
    "  \"security_deposit\",\n",
    "  \"cleaning_fee\",\n",
    "  \"extra_people\",\n",
    "  \"number_of_reviews\",\n",
    "  \"square_feet\",\n",
    "  \"review_scores_rating\"]\n",
    "\n",
    "categorical_features = [\"room_type\",\n",
    "  \"host_is_superhost\",\n",
    "  \"cancellation_policy\",\n",
    "  \"state\",\n",
    "  \"instant_bookable\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imputed_continuous_features = ['imp_{}'.format(x) for x in continuous_features]\n",
    "\n",
    "feature_extractor2_tf = FeatureExtractor(input_scalars=continuous_features, \n",
    "                                         output_vector='imputed_features', \n",
    "                                         output_vector_items=imputed_continuous_features)\n",
    "\n",
    "impute_security_deposit_tf = Imputer(strategy='mean', axis=0)\n",
    "impute_security_deposit_tf.mlinit(prior_tf=feature_extractor2_tf)\n",
    "\n",
    "impute_pipeline = Pipeline([\n",
    "        (feature_extractor2_tf.name, feature_extractor2_tf),\n",
    "        (impute_security_deposit_tf.name, impute_security_deposit_tf)\n",
    "    ])\n",
    "impute_pipeline.mlinit()\n",
    "\n",
    "# Consider doing this via a feature union\n",
    "df2 = df.join(pd.DataFrame(impute_pipeline.fit_transform(df), columns=feature_extractor2_tf.output_vector_items))\n",
    "\n",
    "all_features = imputed_continuous_features + categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Split data into training and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First filter out outlier prices\n",
    "df2 = df2[(df2.price>=50)&(df2.price<=500)]\n",
    "\n",
    "# Split into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df2[all_features], df2[['price']], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Continous Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_extractor_tf = FeatureExtractor(input_scalars=imputed_continuous_features, \n",
    "                                        output_vector='unscaled_cont_features', \n",
    "                                        output_vector_items=[\"scaled_{}\".format(x) for x in imputed_continuous_features])\n",
    "\n",
    "standard_scaler_tf = StandardScaler()\n",
    "standard_scaler_tf.mlinit(prior_tf=feature_extractor_tf)\n",
    "\n",
    "standard_scaler_pipeline = Pipeline([(feature_extractor_tf.name, feature_extractor_tf),\n",
    "                            (standard_scaler_tf.name, standard_scaler_tf)])\n",
    "standard_scaler_pipeline.mlinit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Categorical Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Need to fix scikit's One-Hot-Encoder to drop the last column of a matrix if we're using it for ML\n",
    "def _create_le_one_hot_pipeline(feature_name):\n",
    "    feature_extractor3_tf = FeatureExtractor(input_scalars=[feature_name], \n",
    "                                             output_vector='{}_label'.format(feature_name), \n",
    "                                             output_vector_items=[feature_name])\n",
    "                                             \n",
    "\n",
    "    # Label Encoder for x1 Label \n",
    "    label_encoder_tf = LabelEncoder(input_features = feature_extractor3_tf.output_vector_items, output_features='{}_label_le'.format(feature_name))\n",
    "\n",
    "    # Reshape the output of the LabelEncoder to N-by-1 array\n",
    "    reshape_le_tf = ReshapeArrayToN1()\n",
    "\n",
    "    # Vector Assembler for x1 One Hot Encoder\n",
    "    one_hot_encoder_tf = OneHotEncoder(sparse=False)\n",
    "    one_hot_encoder_tf.mlinit(prior_tf=label_encoder_tf)\n",
    "\n",
    "    one_hot_encoder_pipeline_x0 = Pipeline([\n",
    "                                             (feature_extractor3_tf.name, feature_extractor3_tf),\n",
    "                                             (label_encoder_tf.name, label_encoder_tf),\n",
    "                                             (reshape_le_tf.name, reshape_le_tf),\n",
    "                                             (one_hot_encoder_tf.name, one_hot_encoder_tf)\n",
    "                                            ])\n",
    "    \n",
    "    one_hot_encoder_pipeline_x0.mlinit()\n",
    "    \n",
    "    return one_hot_encoder_pipeline_x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oh_pipelines = [_create_le_one_hot_pipeline(x) for x in categorical_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oh_fes = [x.steps[-1][1] for x in oh_pipelines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Assemble our features and feature pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_union = FeatureUnion([\n",
    "        (standard_scaler_pipeline.name, standard_scaler_pipeline)\n",
    "    ] + [(x.name, x) for x in oh_pipelines])\n",
    "feature_union.mlinit()\n",
    "\n",
    "feature_union.fit(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Define our linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put all of the categorical features into a list\n",
    "oh_features_lists = [[y[1].output_features for y in x.steps if y[1].op == 'one_hot_encoder'] for x in oh_pipelines]\n",
    "oh_features = [item for sublist in oh_features_lists for item in sublist]\n",
    "oh_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vector Assembler, for serialization purposes only\n",
    "feature_extractor_lr_model_tf = FeatureExtractor(input_vectors=[feature_extractor_tf] + oh_fes,\n",
    "                                                 output_vector='input_features',\n",
    "                                                 output_vector_items=[standard_scaler_tf.output_features] + oh_features\n",
    "                                                )\n",
    "feature_extractor_lr_model_tf.skip_fit_transform = True\n",
    "\n",
    "# Define our linear regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.mlinit(input_features='input_features', prediction_column='price_prediction')\n",
    "\n",
    "lr_model_pipeline = Pipeline([\n",
    "        (feature_extractor_lr_model_tf.name, feature_extractor_lr_model_tf),\n",
    "        (lr_model.name, lr_model)\n",
    "    ])\n",
    "lr_model_pipeline.mlinit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline([(feature_union.name, feature_union),\n",
    "                            (lr_model_pipeline.name, lr_model_pipeline)])\n",
    "\n",
    "model_pipeline.mlinit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_pipeline.serialize_to_bundle('/tmp', 'scikit-airbnb.lr', init=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Define our Random Forest Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vector Assembler, for serialization purposes only\n",
    "feature_extractor_rf_model_tf = FeatureExtractor(input_vectors=[feature_extractor_tf],\n",
    "                                                 output_vector='input_features', \n",
    "                                                 output_vector_items=imputed_continuous_features)\n",
    "feature_extractor_rf_model_tf.skip_fit_transform = True\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor(max_depth=4, n_estimators=11)\n",
    "rf.mlinit(input_features=feature_extractor_rf_model_tf.output_vector, prediction_column='price_prediction', feature_names=imputed_continuous_features)\n",
    "\n",
    "rf_model_pipeline = Pipeline([\n",
    "        (feature_extractor_rf_model_tf.name, feature_extractor_rf_model_tf),\n",
    "        (rf.name, rf)\n",
    "    ])\n",
    "rf_model_pipeline.mlinit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_model_pipeline.fit(X_train[imputed_continuous_features], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Fit our pipeline and regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "params = {\n",
    "    \"{}__max_depth\".format(rf.name): [5, 10],\n",
    "    \"{}__n_estimators\".format(rf.name): [10, 15, 20]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(estimator=rf_model_pipeline, param_grid=params, n_jobs=-1)\n",
    "rf_grid.fit(X_train[imputed_continuous_features], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_rf = rf_grid.best_params_\n",
    "best_max_depth = best_rf[\"{}__max_depth\".format(rf.name)]\n",
    "best_n_estimators = best_rf[\"{}__n_estimators\".format(rf.name)]\n",
    "\n",
    "rf_optimal = RandomForestRegressor(max_depth=best_max_depth, n_estimators=best_n_estimators)\n",
    "\n",
    "rf_optimal.mlinit(input_features=feature_extractor_rf_model_tf.output_vector, \n",
    "                  prediction_column='price_prediction', \n",
    "                  feature_names=imputed_continuous_features)\n",
    "\n",
    "rf_model_pipeline = Pipeline([\n",
    "        (feature_extractor_rf_model_tf.name, feature_extractor_rf_model_tf),\n",
    "        (rf_optimal.name, rf_optimal)\n",
    "    ])\n",
    "rf_model_pipeline.mlinit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_pipeline.fit(X_train, y_train)\n",
    "#rf_model_pipeline.fit(X_train[imputed_continuous_features], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_pipeline.steps[-1][1].steps[0][1].input_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Serialize our pipelines to bundle.ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Serialize the linear regression model\n",
    "model_pipeline.serialize_to_bundle('/tmp', 'scikit-airbnb.lr', init=True)\n",
    "\n",
    "# Serialiaze the random forest model\n",
    "#rf_model_pipeline.serialize_to_bundle('/tmp', 'scikit-airbnb.rf', init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [{\n",
    "      \"name\": \"state\",\n",
    "      \"type\": \"string\"\n",
    "    }, {\n",
    "      \"name\": \"imp_bathrooms\",\n",
    "      \"type\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"imp_square_feet\",\n",
    "      \"type\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"imp_bedrooms\",\n",
    "      \"type\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"imp_security_deposit\",\n",
    "      \"type\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"imp_cleaning_fee\",\n",
    "      \"type\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"imp_extra_people\",\n",
    "      \"type\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"imp_number_of_reviews\",\n",
    "      \"type\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"imp_review_scores_rating\",\n",
    "      \"type\": \"double\"\n",
    "    }, {\n",
    "      \"name\": \"room_type\",\n",
    "      \"type\": \"string\"\n",
    "    }, {\n",
    "      \"name\": \"host_is_superhost\",\n",
    "      \"type\": \"string\"\n",
    "    }, {\n",
    "      \"name\": \"cancellation_policy\",\n",
    "      \"type\": \"string\"\n",
    "    }, {\n",
    "      \"name\": \"instant_bookable\",\n",
    "      \"type\": \"string\"\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = [x['name'] for x in a]\n",
    "df_tet = pd.DataFrame([(\"NY\", 2.0, 1250.0, 3.0, 50.0, 30.0, 2.0, 56.0, 90.0, \"Entire home/apt\", \"1.0\", \"strict\", \"1.0\")], columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_pipeline.predict(df_tet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
