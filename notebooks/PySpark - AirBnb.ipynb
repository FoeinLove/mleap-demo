{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLeap.deploy() Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set-up running Spark 2.0 (required for this demo) from a Jupyter notebook, follow these [instructions](https://github.com/combust-ml/mleap/wiki/Setting-up-a-Spark-2.0-notebook-with-MLeap-and-Toree).\n",
    "\n",
    "This demo will show you how to:\n",
    "1. Load the research dataset from s3\n",
    "2. Construct a feature transformer pipeline using commonly available transformers in Spark\n",
    "3. Deploy your model to a public model server hosted on the combust.ml cloud using .deploy()\n",
    "\n",
    "NOTE: To run the actual deploy step you have to either:\n",
    "1. Get a key from combust.ml - it's easy, just email us!\n",
    "2. Fire up the combust cloud server on your local machine - also easy, send us an email and we'll send you a docker image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for the demo was pulled together from individual cities' data found [here](http://insideairbnb.com/get-the-data.html). We've also gone ahead and pulled the individual datasets and relevant features into this [research dataset](https://s3-us-west-2.amazonaws.com/mleap-demo/datasources/airbnb.avro) stored as avro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Load libraries and data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from mleap.pyspark.spark_support import SimpleSparkSerializer\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"com.databricks.spark.avro\").load(\"file:\/\/\/\/tmp/airbnb.avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389255\n",
      "321588\n"
     ]
    }
   ],
   "source": [
    "datasetFiltered = df.filter(\"price >= 50 AND price <= 750 and bathrooms > 0.0\")\n",
    "print(df.count())\n",
    "print(datasetFiltered.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Standardize the data for our demo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|       square_feet|             price|          bedrooms|         bathrooms|     cleaning_fee|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|            321588|            321588|            321588|            321588|           321588|\n",
      "|   mean| 546.7441757777032|131.54961006007687|1.3352426085550455| 1.199068373198005|37.64188340360959|\n",
      "| stddev|363.39839582374066| 90.10912788720098|0.8466586601060778|0.4830590051262673|42.64237791484579|\n",
      "|    min|             104.0|              50.0|               0.0|               0.5|              0.0|\n",
      "|    max|           32292.0|             750.0|              10.0|               8.0|            700.0|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasetFiltered.registerTempTable(\"df\")\n",
    "\n",
    "datasetImputed = spark.sql(\"\"\"\n",
    "    select\n",
    "        id,\n",
    "        city,\n",
    "        case when state in('NY', 'CA', 'London', 'Berlin', 'TX' ,'IL', 'OR', 'DC', 'WA')\n",
    "            then state\n",
    "            else 'Other'\n",
    "        end as state,\n",
    "        space,\n",
    "        price,\n",
    "        bathrooms,\n",
    "        bedrooms,\n",
    "        room_type,\n",
    "        host_is_superhost,\n",
    "        cancellation_policy,\n",
    "        case when security_deposit is null\n",
    "            then 0.0\n",
    "            else security_deposit\n",
    "        end as security_deposit,\n",
    "        price_per_bedroom,\n",
    "        case when number_of_reviews is null\n",
    "            then 0.0\n",
    "            else number_of_reviews\n",
    "        end as number_of_reviews,\n",
    "        case when extra_people is null\n",
    "            then 0.0\n",
    "            else extra_people\n",
    "        end as extra_people,\n",
    "        instant_bookable,\n",
    "        case when cleaning_fee is null\n",
    "            then 0.0\n",
    "            else cleaning_fee\n",
    "        end as cleaning_fee,\n",
    "        case when review_scores_rating is null\n",
    "            then 80.0\n",
    "            else review_scores_rating\n",
    "        end as review_scores_rating,\n",
    "        case when square_feet is not null and square_feet > 100\n",
    "            then square_feet\n",
    "            when (square_feet is null or square_feet <=100) and (bedrooms is null or bedrooms = 0)\n",
    "            then 350.0\n",
    "            else 380 * bedrooms\n",
    "        end as square_feet,\n",
    "        case when bathrooms >= 2\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as n_bathrooms_more_than_two\n",
    "    from df\n",
    "    where bedrooms is not null\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "datasetImputed.select(\"square_feet\", \"price\", \"bedrooms\", \"bathrooms\", \"cleaning_fee\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Take a look at some summary statistics of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+---------+---------+\n",
      "|        state|    n|avg_price|max_price|\n",
      "+-------------+-----+---------+---------+\n",
      "|           NY|48362|   146.75|    750.0|\n",
      "|           CA|44716|   158.76|    750.0|\n",
      "|Île-de-France|40732|   107.74|    750.0|\n",
      "|       London|17532|   117.71|    750.0|\n",
      "|          NSW|14416|   167.96|    750.0|\n",
      "|       Berlin|13098|    81.01|    650.0|\n",
      "|Noord-Holland| 8890|   128.56|    750.0|\n",
      "|          VIC| 8636|   144.49|    750.0|\n",
      "|North Holland| 7636|   134.60|    700.0|\n",
      "|           IL| 7544|   141.85|    750.0|\n",
      "|           ON| 7186|   129.05|    750.0|\n",
      "|           TX| 6702|   196.59|    750.0|\n",
      "|           WA| 5858|   132.48|    750.0|\n",
      "|    Catalonia| 5748|   106.39|    720.0|\n",
      "|           BC| 5522|   133.14|    750.0|\n",
      "|           DC| 5476|   136.56|    720.0|\n",
      "|       Québec| 5116|   104.98|    700.0|\n",
      "|    Catalunya| 4570|    99.36|    675.0|\n",
      "|       Veneto| 4486|   131.71|    700.0|\n",
      "|           OR| 4330|   114.02|    700.0|\n",
      "+-------------+-----+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most popular cities (original dataset)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        state,\n",
    "        count(*) as n,\n",
    "        cast(avg(price) as decimal(12,2)) as avg_price,\n",
    "        max(price) as max_price\n",
    "    from df\n",
    "    group by state\n",
    "    order by count(*) desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+---------+---------+\n",
      "|               city|  n|avg_price|max_price|\n",
      "+-------------------+---+---------+---------+\n",
      "|         Palm Beach| 56|   372.11|    701.0|\n",
      "|        Watsonville| 78|   307.85|    670.0|\n",
      "|  Pacific Palisades| 34|   295.18|    695.0|\n",
      "|             Malibu|302|   280.42|    750.0|\n",
      "|      Bilgola Beach| 30|   261.13|    601.0|\n",
      "|      Playa Del Rey| 34|   255.76|    599.0|\n",
      "|             Avalon| 80|   255.65|    701.0|\n",
      "|Sydney Olympic Park| 40|   250.55|    520.0|\n",
      "|           Tamarama|148|   247.45|    750.0|\n",
      "|           Capitola| 72|   246.50|    650.0|\n",
      "|    Manhattan Beach|240|   234.23|    700.0|\n",
      "|       Avalon Beach| 82|   232.98|    620.0|\n",
      "|            Del Mar| 38|   232.84|    650.0|\n",
      "|         Birchgrove| 32|   228.63|    601.0|\n",
      "|          Mona Vale| 52|   227.00|    572.0|\n",
      "|       Venice Beach| 62|   224.45|    699.0|\n",
      "|Rancho Palos Verdes| 82|   223.68|    750.0|\n",
      "|      Darling Point| 60|   221.43|    623.0|\n",
      "|    North Curl Curl| 26|   220.77|    550.0|\n",
      "|            Newport|114|   219.61|    750.0|\n",
      "+-------------------+---+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most expensive popular cities (original dataset)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        city,\n",
    "        count(*) as n,\n",
    "        cast(avg(price) as decimal(12,2)) as avg_price,\n",
    "        max(price) as max_price\n",
    "    from df\n",
    "    group by city\n",
    "    order by avg(price) desc\n",
    "\"\"\").filter(\"n>25\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define continous and categorical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 2. Create our feature pipeline and train it on the entire dataset\n",
    "continuous_features = [\"bathrooms\", \"bedrooms\", \"security_deposit\", \"cleaning_fee\", \"extra_people\", \"number_of_reviews\", \"square_feet\", \"review_scores_rating\"]\n",
    "\n",
    "categorical_features = [\"room_type\", \"host_is_superhost\", \"cancellation_policy\", \"instant_bookable\", \"state\"]\n",
    "\n",
    "all_features = continuous_features + categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_imputed = datasetImputed.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Split data into training and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[training_dataset, validation_dataset] = dataset_imputed.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Continous Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "continuous_feature_assembler= VectorAssembler(inputCols=continuous_features, outputCol=\"unscaled_continuous_features\")\n",
    "\n",
    "continuous_feature_scaler = StandardScaler(inputCol=\"unscaled_continuous_features\", outputCol=\"scaled_continuous_features\",\\\n",
    "                                           withStd=True, withMean=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Categorical Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical_feature_indexers = [StringIndexer(inputCol=x, outputCol=\"{}_index\".format(x)) for x in categorical_features]\n",
    "\n",
    "categorical_feature_one_hot_encoders = [OneHotEncoder(inputCol=x.getOutputCol(), outputCol=\"oh_encoder_{}\".format(x.getOutputCol() )) for x in categorical_feature_indexers]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Assemble our features and feature pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
     ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished constructing the pipeline\n"
     ]
    }
   ],
   "source": [
    "estimatorsLr = [continuous_feature_assembler, continuous_feature_scaler] + categorical_feature_indexers+ categorical_feature_one_hot_encoders\n",
    "\n",
    "featurePipeline = Pipeline(stages=estimatorsLr)\n",
    "\n",
    "sparkFeaturePipelineModel = featurePipeline.fit(dataset_imputed)\n",
    "\n",
    "print(\"Finished constructing the pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 7: Train a Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete: Training Linear Regression\n"
     ]
    }
   ],
   "source": [
    "# Create our linear regression model\n",
    "\n",
    "linearRegression = LinearRegression(featuresCol=\"scaled_continuous_features\", labelCol=\"price\", predictionCol=\"price_prediction\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "pipeline_lr = [sparkFeaturePipelineModel] + [linearRegression]\n",
    "\n",
    "sparkPipelineEstimatorLr = Pipeline(stages = pipeline_lr)\n",
    "\n",
    "sparkPipelineLr = sparkPipelineEstimatorLr.fit(dataset_imputed)\n",
    "\n",
    "print(\"Complete: Training Linear Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.1: Train a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete: Training Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "# Create our logistic regression model\n",
    "\n",
    "logisticRegression = LogisticRegression(featuresCol=\"scaled_continuous_features\", labelCol=\"n_bathrooms_more_than_two\", predictionCol=\"n_bathrooms_more_than_two_prediction\", maxIter=10)\n",
    "\n",
    "pipeline_log_r = [sparkFeaturePipelineModel] + [logisticRegression]\n",
    "\n",
    "sparkPipelineEstimatorLogr = Pipeline(stages = pipeline_log_r)\n",
    "\n",
    "sparkPipelineLogr = sparkPipelineEstimatorLogr.fit(dataset_imputed)\n",
    "\n",
    "print(\"Complete: Training Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Serialize the model to Bundle.ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparkPipelineLr.serializeToBundle(\"jar:file:/tmp/pyspark.lr.zip\", sparkPipelineLr.transform(dataset_imputed))\n",
    "sparkPipelineLogr.serializeToBundle(\"jar:file:/tmp/pyspark.logr.zip\", dataset=sparkPipelineLogr.transform(dataset_imputed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9 (Optional): Deserialize from Bundle.ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparkPipelineLr = PipelineModel.deserializeFromBundle(\"jar:file:/tmp/pyspark.lr.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10 (Optional): Deploy to ModelServer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python bindings for .deploy() are coming soon. For now, you may have to write a few lines of scala - demo for that can be found [here](https://github.com/combust-ml/mleap-demo/blob/master/lending-club/notebooks/airbnb-price-regression.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree_pyspark - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark_pyspark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
